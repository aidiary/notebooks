{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/02-intermediate/language_model/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mcifar-10-batches-py\u001b[m\u001b[m/    \u001b[1m\u001b[36mmnist\u001b[m\u001b[m/                  \u001b[1m\u001b[36mraw\u001b[m\u001b[m/\r\n",
      "cifar-10-python.tar.gz  picasso.jpg             train.txt\r\n",
      "dancing.jpg             \u001b[1m\u001b[36mprocessed\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-01-20 16:25:02--  https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/02-intermediate/language_model/data/train.txt\n",
      "Resolving raw.githubusercontent.com... 151.101.72.133\n",
      "Connecting to raw.githubusercontent.com|151.101.72.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 5101618 (4.9M) [text/plain]\n",
      "Saving to: ‘data/train.txt.1’\n",
      "\n",
      "train.txt.1         100%[===================>]   4.87M  3.53MB/s    in 1.4s    \n",
      "\n",
      "2018-01-20 16:25:04 (3.53 MB/s) - ‘data/train.txt.1’ saved [5101618/5101618]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/02-intermediate/language_model/data/train.txt -P data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "less data/train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Me': 0, 'Hello': 1}\n",
      "{0: 'Me', 1: 'Hello'}\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "d = Dictionary()\n",
    "d.add_word('Me')\n",
    "d.add_word('Hello')\n",
    "print(d.word2idx)\n",
    "print(d.idx2word)\n",
    "print(len(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Corpus(object):\n",
    "    def __init__(self, path='./data'):\n",
    "        self.dictionary = Dictionary()\n",
    "\n",
    "    def get_data(self, path, batch_size=20):\n",
    "        # add words to the dictionary\n",
    "        with open(path, 'r') as f:\n",
    "            tokens = 0\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                tokens += len(words)\n",
    "                for word in words:\n",
    "                    self.dictionary.add_word(word)\n",
    "        \n",
    "        # tokenize the file content\n",
    "        ids = torch.LongTensor(tokens)\n",
    "        token = 0\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                words = line.split() + ['<eos>']\n",
    "                for word in words:\n",
    "                    ids[token] = self.dictionary.word2idx[word]\n",
    "                    token += 1\n",
    "\n",
    "        # バッチサイズで割り切れるサイズにする\n",
    "        num_batches = ids.size(0) // batch_size\n",
    "        ids = ids[:num_batches * batch_size]\n",
    "        return ids.view(batch_size, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids: torch.Size([20, 46479])\n",
      "vocab_size: 10000\n",
      "num_batches: 1549\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "#from data_utils import Dictionary, Corpus\n",
    "\n",
    "# hyper parameters\n",
    "embed_size = 128\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "num_epochs = 5\n",
    "num_samples = 1000\n",
    "batch_size = 20\n",
    "seq_length = 30\n",
    "learning_rate = 0.002\n",
    "\n",
    "# Load Penn Treebank Dataset\n",
    "train_path = './data/train.txt'\n",
    "sample_path = './sample.txt'\n",
    "\n",
    "corpus = Corpus()\n",
    "# インデックスに変換したコーパス\n",
    "ids = corpus.get_data(train_path, batch_size)  # (20, 46479)\n",
    "vocab_size = len(corpus.dictionary)  # 10000\n",
    "num_batches = ids.size(1) // seq_length  # 1549\n",
    "print('ids:', ids.size())\n",
    "print('vocab_size:', vocab_size)\n",
    "print('num_batches:', num_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RNN based language model\n",
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
    "        super(RNNLM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.init_weight()\n",
    "    \n",
    "    def init_weight(self):\n",
    "        self.embed.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
    "    \n",
    "    def forward(self, x, h):  # [20, 30]\n",
    "        print('x:', x.size())\n",
    "        print('h:', h[0].size())\n",
    "        print('c:', h[1].size())\n",
    "\n",
    "        # embed word ids to vectors\n",
    "        x = self.embed(x)  # [20, 30, 128]\n",
    "        print('embed:', x.size())\n",
    "\n",
    "        # forward propagate RNN\n",
    "        out, h = self.lstm(x, h)  # [20, 30, 1024]\n",
    "        out = out.contiguous().view(out.size(0) * out.size(1), out.size(2))\n",
    "        out = self.linear(out)        \n",
    "\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers)\n",
    "#model.cuda()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([20, 30])\n",
      "h: torch.Size([1, 20, 1024])\n",
      "c: torch.Size([1, 20, 1024])\n",
      "embed: torch.Size([20, 30, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       "  1.2109e-02 -4.2164e-03 -1.9561e-02  ...  -2.2535e-03  1.3184e-02  1.7042e-02\n",
       "  2.0521e-02 -1.2563e-02 -4.0352e-02  ...  -1.2311e-03  1.7413e-02  2.7035e-02\n",
       "  2.2108e-02 -2.7091e-02 -5.1551e-02  ...  -9.8233e-03  1.4894e-02  2.3743e-02\n",
       "                 ...                   ⋱                   ...                \n",
       "  1.5702e-02 -1.9350e-02 -6.8386e-02  ...  -3.6951e-03  3.1976e-02  3.7417e-02\n",
       "  1.3191e-02 -2.7237e-02 -7.3997e-02  ...  -5.2689e-03  4.0979e-02  3.3340e-02\n",
       "  2.4224e-02 -2.9213e-02 -7.2764e-02  ...   9.0177e-03  2.9738e-02  3.8463e-02\n",
       " [torch.FloatTensor of size 600x10000], (Variable containing:\n",
       "  ( 0  ,.,.) = \n",
       "  1.00000e-02 *\n",
       "   -1.6489 -0.4631 -2.1975  ...   1.1436 -3.2848 -1.6932\n",
       "   -0.4430  0.0342 -1.6100  ...   0.8339 -2.7952 -2.0236\n",
       "   -0.5340 -0.5879 -1.9551  ...   1.1781 -2.7270 -1.9951\n",
       "             ...             ⋱             ...          \n",
       "   -0.6378 -1.6057 -1.5392  ...   0.4422 -3.1842 -1.6217\n",
       "   -1.1324 -0.6035 -2.3166  ...   0.6852 -2.6370 -1.9747\n",
       "   -0.4819 -0.5073 -1.1526  ...   0.2920 -3.2456 -2.1341\n",
       "  [torch.FloatTensor of size 1x20x1024], Variable containing:\n",
       "  ( 0  ,.,.) = \n",
       "  1.00000e-02 *\n",
       "   -3.3623 -0.9232 -4.4940  ...   2.2906 -6.5448 -3.3243\n",
       "   -0.9066  0.0683 -3.2917  ...   1.6573 -5.5193 -3.9614\n",
       "   -1.0873 -1.1723 -4.0166  ...   2.3650 -5.4462 -3.9202\n",
       "             ...             ⋱             ...          \n",
       "   -1.2896 -3.1974 -3.1445  ...   0.8813 -6.3159 -3.1986\n",
       "   -2.2821 -1.1934 -4.7339  ...   1.3766 -5.2770 -3.8497\n",
       "   -0.9760 -1.0122 -2.3656  ...   0.5813 -6.4086 -4.1912\n",
       "  [torch.FloatTensor of size 1x20x1024]))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = Variable(ids[:, 0:30])\n",
    "targets = Variable(ids[:, 1:31])\n",
    "states = (Variable(torch.zeros(num_layers, batch_size, hidden_size)),\n",
    "          Variable(torch.zeros(num_layers, batch_size, hidden_size)))\n",
    "model(inputs, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training\n",
    "for epoch in range(num_epochs):\n",
    "    # initial hidden and memory states\n",
    "    states = (Variable(torch.zeros(num_layers, batch_size, hidden_size)),\n",
    "              Variable(torch.zeros(num_layers, batch_size, hidden_size)))\n",
    "    \n",
    "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
    "        # get batch inputs and targets\n",
    "        # 入力単語系列に対して1つずらした単語系列が出力となるように学習\n",
    "        # in: [0:30], out: [1:31]\n",
    "        # in: [1:31], out: [2:32]\n",
    "        inputs = Variable(ids[:, i:i+seq_length])\n",
    "        targets = Variable(ids[:, (i+1):(i+1)+seq_length])\n",
    "        \n",
    "        model.zero_grad()\n",
    "        states = detach(states)\n",
    "        outputs, states = model(inputs, states)\n",
    "        loss = criterion(outputs, targets.view(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.paramters(), 0.5)\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
