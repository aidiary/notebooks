{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial\n",
    "https://github.com/yunjey/pytorch-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自動微分 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create tensors\n",
    "# Variableは自動微分の対象になる\n",
    "x = Variable(torch.Tensor([1]), requires_grad=True)\n",
    "w = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "b = Variable(torch.Tensor([3]), requires_grad=True)\n",
    "\n",
    "# build a computational graph\n",
    "y = w * x + b  # y = 2 * x + 3\n",
    "\n",
    "# compute gradients\n",
    "y.backward()\n",
    "\n",
    "# print gradients\n",
    "print(x.grad)  # dy/dx = w\n",
    "print(w.grad)  # dy/dw = x\n",
    "print(b.grad)  # dy/db = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自動微分 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: Parameter containing:\n",
      "-0.0213  0.3410  0.4479\n",
      "-0.2651  0.4823 -0.2477\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "b: Parameter containing:\n",
      " 0.2191\n",
      " 0.0330\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "loss: 0.42183035612106323\n",
      "dL/dw: Variable containing:\n",
      " 0.2508  0.3263 -0.0476\n",
      " 0.4193  0.6737 -0.2881\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "dL/db: Variable containing:\n",
      "-0.0772\n",
      "-0.1106\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "*** by hand\n",
      "\n",
      "-0.0238  0.3377  0.4484\n",
      "-0.2693  0.4755 -0.2448\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "\n",
      " 0.2199\n",
      " 0.0341\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "*** by step()\n",
      "Parameter containing:\n",
      "-0.0238  0.3377  0.4484\n",
      "-0.2693  0.4755 -0.2448\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "Parameter containing:\n",
      " 0.2199\n",
      " 0.0341\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# autograd 2\n",
    "x = Variable(torch.randn(5, 3))\n",
    "y = Variable(torch.randn(5, 2))\n",
    "\n",
    "# linear layer\n",
    "linear = nn.Linear(3, 2)\n",
    "print('w:', linear.weight)\n",
    "print('b:', linear.bias)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# forward\n",
    "pred = linear(x)\n",
    "\n",
    "# compute loss\n",
    "loss = criterion(pred, y)\n",
    "print('loss:', loss.data[0])\n",
    "\n",
    "# backpropagation\n",
    "loss.backward()\n",
    "\n",
    "# print out the gradients\n",
    "print('dL/dw:', linear.weight.grad)\n",
    "print('dL/db:', linear.bias.grad)\n",
    "\n",
    "print('*** by hand')\n",
    "# optimizer.step()の中身は下の計算をしている\n",
    "# 計算結果が一致する\n",
    "print(linear.weight.data.sub(0.01 * linear.weight.grad.data))\n",
    "print(linear.bias.data.sub(0.01 * linear.bias.grad.data))\n",
    "\n",
    "# gradient descent\n",
    "optimizer.step()\n",
    "\n",
    "print('*** by step()')\n",
    "print(linear.weight)\n",
    "print(linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ndarray <=> tensor の相互変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.LongTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "\n",
      " 1  2\n",
      " 3  4\n",
      "[torch.LongTensor of size 2x2]\n",
      "\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Loading data from numpy\n",
    "a = np.array([[1, 2], [3, 4]])\n",
    "b = torch.from_numpy(a)  # ndarray => tensor\n",
    "c = b.numpy()            # tensor  => ndarray\n",
    "print(type(a))\n",
    "print(type(b))\n",
    "print(type(c))\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DatasetとDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "torch.Size([3, 32, 32])\n",
      "6\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# Implementing the input pipeline\n",
    "# Download and construct dataset\n",
    "# Datasetクラスのサブクラス（__getitem__と__len__が実装されている）\n",
    "train_dataset = dsets.CIFAR10(root='./data',\n",
    "                              train=True,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              download=True)\n",
    "\n",
    "# select one data pair\n",
    "# ディスクからデータを読む\n",
    "image, label = train_dataset[0]\n",
    "print(image.size())\n",
    "print(label)\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 32, 32]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# data loader\n",
    "# Datasetを渡すとバッチ単位でファイルからデータを取り出せる\n",
    "# num_workersを指定することでマルチプロセッサで並列に読み出せる？\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=2)\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "# ミニバッチ単位で画像とラベルをロード\n",
    "images, labels = data_iter.next()\n",
    "print(images.size(), labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 32, 32]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# 実際はiterを使わなくてforループでOK\n",
    "for images, labels in train_loader:\n",
    "    print(images.size(), labels.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カスタムデータセット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `__getitem__()` と `__len__()` を定義する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(data.Dataset):\n",
    "    def __init__(self):\n",
    "        # initialize file path or list of file names.\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open)\n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform)\n",
    "        # 3. Return a data pair (e.g. image and label)\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        # return total size of your dataset\n",
    "        return 0\n",
    "\n",
    "custom_dataset = CustomDataset()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n",
    "                                           batch_size=100,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習済みモデルを使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download and load pretrained resnet\n",
    "resnet = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet (\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (relu): ReLU (inplace)\n",
       "  (maxpool): MaxPool2d (size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1))\n",
       "  (layer1): Sequential (\n",
       "    (0): BasicBlock (\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (1): BasicBlock (\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential (\n",
       "    (0): BasicBlock (\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential (\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock (\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential (\n",
       "    (0): BasicBlock (\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential (\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock (\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential (\n",
       "    (0): BasicBlock (\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential (\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock (\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d (size=7, stride=7, padding=0, ceil_mode=False, count_include_pad=True)\n",
       "  (fc): Linear (512 -> 1000)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finetuneしたいときはパラメータを固定する\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# topのfc層を置き換える\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 新しく追加したfc層のパラメータは更新対象\n",
    "for param in resnet.fc.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "images = Variable(torch.randn(10, 3, 256, 256))\n",
    "outputs = resnet(images)\n",
    "print(outputs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "# 74番目のクラスに分類された\n",
    "print(np.argmax(outputs[0].data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルのセーブとロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet (\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (relu): ReLU (inplace)\n",
      "  (maxpool): MaxPool2d (size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1))\n",
      "  (layer1): Sequential (\n",
      "    (0): BasicBlock (\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "    (1): BasicBlock (\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential (\n",
      "    (0): BasicBlock (\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (downsample): Sequential (\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock (\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential (\n",
      "    (0): BasicBlock (\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (downsample): Sequential (\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock (\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential (\n",
      "    (0): BasicBlock (\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (downsample): Sequential (\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock (\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d (size=7, stride=7, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  (fc): Linear (512 -> 100)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# save and load the entire model\n",
    "torch.save(resnet, 'model.pkl')\n",
    "model = torch.load('model.pkl')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save and load only the model parameters\n",
    "# こちらのほうが推薦されている\n",
    "# ファイルサイズは変わらない？\n",
    "torch.save(resnet.state_dict(), 'params.pkl')\n",
    "resnet.load_state_dict(torch.load('params.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/60], Loss: 9.3287\n",
      "Epoch [20/60], Loss: 1.9054\n",
      "Epoch [30/60], Loss: 0.6859\n",
      "Epoch [40/60], Loss: 0.4845\n",
      "Epoch [50/60], Loss: 0.4503\n",
      "Epoch [60/60], Loss: 0.4435\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# hyper parameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "# toy dataset\n",
    "# 15 samples, 1 features\n",
    "x_train = np.array([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167,\n",
    "                    7.042, 10.791, 5.313, 7.997, 3.1], dtype=np.float32)\n",
    "\n",
    "y_train = np.array([1.7, 2.76, 2.09, 3.19, 1.694, 1.573, 3.366, 2.596, 2.53, 1.221,\n",
    "                    2.827, 3.465, 1.65, 2.904, 1.3], dtype=np.float32)\n",
    "\n",
    "x_train = x_train.reshape(15, 1)\n",
    "y_train = y_train.reshape(15, 1)\n",
    "\n",
    "# linear regression model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # convert numpy array to torch Variable\n",
    "    # ndarray => Tensor => Variable\n",
    "    inputs = Variable(torch.from_numpy(x_train))\n",
    "    targets = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('Epoch [%d/%d], Loss: %.4f' % (epoch + 1, num_epochs, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzt3Xl41NXZ//H3DUQii6KIFdkSEVdk\nDQpFLYogAi4PitJSW3y0VEWlrUtRXCgKYvVx6U/UJy5FH1OtG4oFrQsiiEoJCMpiESRIBBVQtkYw\nkPv3x8SQGSZkkkzyneXzui6u5Jw5+c7NEO6cnDnf+5i7IyIiqaVe0AGIiEj8KbmLiKQgJXcRkRSk\n5C4ikoKU3EVEUpCSu4hICoo5uZtZfTP7yMz+EeWxEWa2wcwWlf65LL5hiohIVTSowtjRwHLggAoe\n/7u7X1XzkEREpKZimrmbWWtgEPBY7YYjIiLxEOvM/X7gBqDpPsacb2anAiuA37v72sgBZjYSGAnQ\nuHHj7sccc0wVwxURSW8LFizY6O4tKhtXaXI3s8HAN+6+wMz6VDDsVeAZd99pZpcDTwKnRw5y91wg\nFyAnJ8fz8/Mre3oRESnHzNbEMi6WZZnewDlmVgA8C5xuZk+XH+Dum9x9Z2nzUaB7FWIVEZE4qzS5\nu/uN7t7a3bOAYcBMd/9l+TFm1rJc8xxCb7yKiEhAqrJbJoyZjQfy3X0acI2ZnQPsAr4FRsQnPBER\nqQ4LquRvtDX34uJiCgsL2bFjRyAxSbjMzExat25NRkZG0KGISCkzW+DuOZWNq/bMvTYUFhbStGlT\nsrKyMLOgw0lr7s6mTZsoLCwkOzs76HBEpIoSqvzAjh07aN68uRJ7AjAzmjdvrt+iRJJUQiV3QIk9\ngejfQiR5JVxyFxFJVTuKd3PvmytYt/n7Wn8uJfcIhYWFnHvuuXTo0IH27dszevRofvjhh6hj161b\nxwUXXFDpNQcOHMjmzZurFc+4ceO45557Kh3XpEmTfT6+efNmHnrooWrFICI191z+Wo655XX+8vZn\nzF6xodafL7mTe14eZGVBvXqhj3l5NbqcuzNkyBDOO+88PvvsM1asWMH27dsZO3bsXmN37drF4Ycf\nzgsvvFDpdWfMmEGzZs1qFFtNKbmLBGPL98VkjZnODS98DMB5XQ5n2Ilta/15kze55+XByJGwZg24\nhz6OHFmjBD9z5kwyMzO55JJLAKhfvz733XcfTzzxBEVFRUyZMoWhQ4dy9tln079/fwoKCujYsSMA\nRUVFXHjhhXTq1ImLLrqIk046iR+3emZlZbFx40YKCgo49thj+c1vfsPxxx9P//79+f770K9njz76\nKD169KBz586cf/75FBUV7TPW1atX06tXL3r06MEtt9xS1r99+3b69u1Lt27dOOGEE3jllVcAGDNm\nDKtWraJLly5cf/31FY4Tkfh55N1VdP7TG2Xt2defxv3DutbJcydvch87FiITYFFRqL+ali5dSvfu\n4ZUTDjjgANq2bcvKlSsB+OCDD3jyySeZOXNm2LiHHnqIgw46iI8//phbbrmFBQsWRH2Ozz77jFGj\nRrF06VKaNWvGiy++CMCQIUOYP38+ixcv5thjj+Xxxx/fZ6yjR4/miiuuYP78+Rx22GFl/ZmZmUyd\nOpWFCxfyzjvvcO211+LuTJo0ifbt27No0SLuvvvuCseJSM19s3UHWWOmM+m1TwH47alHUDBpEG2b\nN6qzGBJqn3uVfPFF1fpj4O5Rd4iU7+/Xrx8HH3zwXmPee+89Ro8eDUDHjh3p1KlT1OfIzs6mS5cu\nAHTv3p2CggIAlixZws0338zmzZvZvn07Z5555j5jnTt3btkPhosvvpg//vGPZbHedNNNzJ49m3r1\n6vHll1/y9ddfR/07RRtX/geFiFTd7f9YxuPvrS5rzx97Bi2aNqzzOJI3ubdtG1qKidZfTccff3xZ\nwvzR1q1bWbt2Le3bt2fBggU0btw46tfGOutt2HDPP3L9+vXLlmVGjBjByy+/TOfOnZkyZQqzZs2q\n9FrRfhDl5eWxYcMGFixYQEZGBllZWVH3qsc6TkRiU7DxP/S5Z1ZZe+zAY/nNqUcEFk/yLstMmACN\nIn7FadQo1F9Nffv2paioiKeeegqA3bt3c+211zJixAgaRT5XhJNPPpnnnnsOgGXLlvHJJ59U6bm3\nbdtGy5YtKS4uJi+G9w169+7Ns88+CxA2fsuWLRx66KFkZGTwzjvvsKb0B2DTpk3Ztm1bpeNEpOqu\nfuajsMT+8bj+gSZ2SObkPnw45OZCu3ZgFvqYmxvqryYzY+rUqTz//PN06NCBo446iszMTCZOnFjp\n11555ZVs2LCBTp06cdddd9GpUycOPPDAmJ/79ttv56STTqJfv37EcojJAw88wOTJk+nRowdbtmwp\n6x8+fDj5+fnk5OSQl5dXdq3mzZvTu3dvOnbsyPXXX1/hOBGJ3ZIvt5A1ZjqvLl4HwD1DO1MwaRAH\nZAZfjymhCoctX76cY489NpB4amr37t0UFxeTmZnJqlWr6Nu3LytWrGC//fYLOrQaSeZ/E5HaUlLi\nDMv9kH8VfAvAQY0y+ODGvmRm1K/1507KwmHJrKioiNNOO43i4mLcnYcffjjpE7uI7O39VRv5xaPz\nytpPjMjh9GN+EmBE0Sm5x0nTpk3RsYEiqat4dwln3PsuazaFtmAfc1hTpl9zCvXrJWYNJiV3EZFK\nvL5kPZc/vbCs/cLlvcjJ2ntLdCKJObmbWX0gH/jS3QdHPNYQeIrQ2ambgIvcvSCOcYqI1Lnvf9hN\n19vfYEdxCQCnHtWCJy/pkRQVU6sycx9N6GzUA6I8dinwnbsfaWbDgLuAi+IQn4hIIP427wtumrpn\nS/M/f3cqRx/WNMCIqiam5G5mrYFBwATgD1GGnAuMK/38BeBBMzPX/ewikmQ2F/1Al/FvlrWHdm/N\n3UM7BxhR9cS6z/1+4AagpILHWwFrAdx9F7AFaB45yMxGmlm+meVv2FD7JS+ro379+nTp0qXsT0FB\nAfn5+VxzzTUAzJo1i/fff79s/Msvv8yyZcuq/DwVlej9sT/WcsIiEj8PzvwsLLHPueG0pEzsEMPM\n3cwGA9+4+wIz61PRsCh9e83a3T0XyIXQPvcqxFln9t9/fxYtWhTWl5WVRU5OaFvprFmzaNKkCT/9\n6U+BUHIfPHgwxx13XFzjiLWcsIjU3FdbdtDzzrfL2qNOa8/1Zyb3jX2xzNx7A+eYWQHwLHC6mT0d\nMaYQaANgZg2AA4Fv4xhnoGbNmsXgwYMpKCjgkUce4b777qNLly68++67TJs2jeuvv54uXbqwatUq\nVq1axYABA+jevTunnHIKn34aqgpXUYneipQvJzxlyhSGDBnCgAED6NChAzfccEPZuDfeeINevXrR\nrVs3hg4dyvbt22vnRRBJUbe9siQssS+4+YykT+wQw8zd3W8EbgQonblf5+6/jBg2Dfg18AFwATCz\npuvtf3p1KcvWba3JJfZy3OEHcNvZx+9zzPfff19WtTE7O5upU6eWPZaVlcXll19OkyZNuO666wA4\n55xzGDx4cNkSSt++fXnkkUfo0KED8+bN48orr2TmzJllJXp/9atfMXny5CrHvmjRIj766CMaNmzI\n0UcfzdVXX83+++/PHXfcwVtvvUXjxo256667uPfee7n11lurfH2RdLNqw3b6/s+7Ze1bBx/Hf5+c\nHWBE8VXtfe5mNh7Id/dpwOPA/5nZSkIz9mFxiq/ORVuWidX27dt5//33GTp0aFnfzp07gYpL9Maq\nb9++ZbVqjjvuONasWcPmzZtZtmwZvXv3BuCHH36gV69e1YpdJF24O1c8vZDXl35V1rfkT2fSpGFq\n3fZTpb+Nu88CZpV+fmu5/h3A0OhfVT2VzbATUUlJCc2aNavwh0NN9sZGlgretWsX7k6/fv145pln\nqn1dkXTyceFmznlwbln7gWFdOLdLqwAjqj3JWxUyIJGlc8u3DzjgALKzs3n++eeB0Axh8eLFQMUl\nemuiZ8+ezJ07t+yUqKKiIlasWBGXa4ukkpIS57zJc8sS+6FNG/LvOwakbGIHJfcqO/vss5k6dSpd\nunRhzpw5DBs2jLvvvpuuXbuyatUq8vLyePzxx+ncuTPHH3982dmkFZXorYkWLVowZcoUfv7zn9Op\nUyd69uxZ9gauiIT8bd4XHHHTDBat3QzAlEt68K+xZ9CwQe1XcAySSv7KPunfRJJV0Q+7OO7Wf5a1\nT2h1IC+P6p2whb5ipZK/IpK2rsxbwIxP9rxhOu7s4xjRO3V2wsRCyV1EUsbG7TvJueOtsL7Vdw5M\nikJf8ZZwyd3d0/IfIhGpNJAkkwH3z+bTr/Zsdnh4eDfOOqFlgBEFK6GSe2ZmJps2baJ58+ZK8AFz\ndzZt2kRmZmbQoYjs0+cbtnN6uZuRAAomDQoomsSRUMm9devWFBYWkqhFxdJNZmYmrVu3DjoMkQpl\njZke1n7xil50b5fYh2jUlYRK7hkZGWRnp9ebHiJSdQvWfMv5D38Q1qfZeriESu4iIpWJnK2/fe3P\naN8iegntdKbkLiJJIfIc0w6HNuHNP/wswIgSm5K7iCQ0dyf7xhlhffPHnkGLpg0r+AoBJXcRSWB/\nnbuaP72656SzszoexsO/7B5gRMlDyV1EEk7x7hI6jH0trG/Z+DNptJ9SVqz0SolIQhn/6jKemLu6\nrH35z9oz5qzkPxmprim5i0hC2L5zFx1v+2dY38oJZ9GgvorXVkcsB2RnArOBhqXjX3D32yLGjADu\nBr4s7XrQ3R+Lb6gikqounTKftz/9pqx9+3kdubhnuwAjSn6xzNx3Aqe7+3YzywDeM7PX3P3DiHF/\nd/er4h+iiKSqb7bu4MSJb4f1pWuhr3ir9PcdD9le2swo/aOKUiJSIz+7+52wxP7Yr3IomDQotRN7\nXh5kZUG9eqGPcTqVLZqY1tzNrD6wADgSmOzu86IMO9/MTgVWAL9397VRrjMSGAnQtm3bagctIsnr\ns6+30e++2WF9aVE6IC8PRo6EoqJQe82aUBtg+PC4P12VTmIys2bAVOBqd19Srr85sN3dd5rZ5cCF\n7n76vq4V7SQmEUltkaUDXh7Vmy5tmgUUTR3Lygol9Ejt2kFBQcyXifUkpiq9De3um4FZwICI/k3u\nvrO0+SiguwxEpMyHn28KS+wNG9SjYNKg9EnsAF98UbX+Gqo0uZtZi9IZO2a2P3AG8GnEmPIV8c8B\nlsczSBFJXlljpjMsd8/+i3ev78O/7zirbp68Dte4K1XRUnQtLVHHMnNvCbxjZh8D84E33f0fZjbe\nzM4pHXONmS01s8XANcCIWolWRJLGq4vXhc3WT2h1IAWTBtGueeO6CeDHNe41a8B9zxp3UAl+wgRo\n1Ci8r1GjUH8tqNKaezxpzV0kNUUr9LXwln4c3Hi/ug0kTmvccZWXB2PHhpZi2rYNJfYqvpka65q7\nkruIxM3/vruKO1/bs2p7XpfDuX9Y12CCqVcvNGOPZAYlJXUfT5zEmtxVfkBEauyHXSUcdXN4oa9P\nbx9AZkb9gCIiNDOONnNPk23YKtogIjVy88ufhCX2a/p2oGDSoGATO9T5Gnei0cxdRKpl645iOo17\nI6xv1cSB1K+XIHeY/riWXcM17mSl5C4iVfbLx+bx3sqNZe27zj+Bi3ok4HLH8OFpk8wjKbmLSMzW\nb/meXnfODOtLi9IBSUhr7iJVkUg3xdSxkya+FZbYp1zSQ4k9gWnmLhKrOi78lCiWr9/KWQ/MCetT\nUk982ucuEqtEvCmmlkUW+vrH1SfTsdWBAUUjoH3uIvFXx4WfgjR35UaGP7ansveB+2ew+Lb+AUYk\nVaXkLhKrNLkpJnK2PueG02hzcKMKRkui0huqIrFK8ZtiXlpYGJbYe2QdRMGkQUrsSUozd5FYpehN\nMSUlzhE3hRf6Wnxrfw5slBFQRBIPSu4iVZFiN8U8OPMz7nljRVn7wpzW/PmCzgFGJPGi5C6ShnYU\n7+aYW14P6wu80JfEVaXJ3cwygdlAw9LxL7j7bRFjGgJPETpebxNwkbsXxD1aEamxG15YzHP5hWXt\n6/ofxVWndwgwIqkNsczcdwKnu/t2M8sA3jOz19z9w3JjLgW+c/cjzWwYcBdwUS3EKyLVtLnoB7qM\nfzOs7/OJA6mXKIW+JK4qTe4eustpe2kzo/RP5J1P5wLjSj9/AXjQzMyDukNKRMJEbm+876LO/FfX\n1gFFI3UhpjV3M6sPLACOBCa7+7yIIa2AtQDuvsvMtgDNgY2ISGCWrdvKwL+odEA6iim5u/tuoIuZ\nNQOmmllHd19Sbki03+v2mrWb2UhgJEDbFLvxQyTRRM7WJw05gWEn6v9duqjSTUzuvhmYBQyIeKgQ\naANgZg2AA4Fvo3x9rrvnuHtOixYtqhWwiOzbzE+/3iuxFzw7imE9s9KukmU6i2W3TAug2N03m9n+\nwBmE3jAtbxrwa+AD4AJgptbbRepeZFJ/Ons7J//hv9OukqXEtizTEniydN29HvCcu//DzMYD+e4+\nDXgc+D8zW0loxj6s1iIWkb1Mmbuaca8uC+srmDQoNFP/MbH/qKgodJetkntKU8lfkSTm7mTfGF46\n4M3fn0qHnzQNNerVg2j/x82gpKQOIpR4i7XkrwqHidSWWj616ZaXl+yV2AsmDdqT2KHiipXa0JDy\nVH5ApDbU4qlNu3aXcOTY18L68m8+g0OaNNx78IQJ4XFASlWylIpp5i5SG8aOrXituwbOmzw3LLG3\narY/BZMGRU/sEPpBkpsbOi3KLPQxN1fr7WlAa+7pIi8v5UrVJrQ4r3VHKx2gQl/pScfsyR5perBz\noOJ4alPk9sZjWx7Aa6NPqW5kkia0LJMOammJQPYhDqc2rfxm+16J/fOJA5XYJSaauaeDNDrYOWHU\n8NSmyKQ+4PjDeOTi7vGOUlKYkns6SJODnRNONU5tmr1iA7964l9hfSr0JdWh5J4OtB0uKUTO1nWI\nhtSEkns6SNGDnVPFk+8XcNu0pWF9mq1LTSm5p4sUO9g5VUTO1h/5ZTcGdGwZUDSSSpTcRQJw40sf\n88y/1ob1abYu8aStkJL6arnGS1W4O1ljpocl9n9cfbISu8SdZu6S2hLoBq4B98/m06+2hfUpqUtt\nUfkBSW1ZWdG3gbZrBwUFdRLCzl27Ofrm18P6/nVTXw49ILNOnl9Si0r+ikDgN3BljZm+V2IvmDSo\neok9gZaXJPHFcsxeG+Ap4DCgBMh19wcixvQBXgFWl3a95O7j4xuqSDUEdAPXxu07ybnjrbC+GhX6\nSqDlJUkOsay57wKudfeFZtYUWGBmb7r7sohxc9x9cPxDFKmBAG7gitzemH1IY965rk/NLrqv+kBK\n7hJFpcnd3dcD60s/32Zmy4FWQGRyF0k8dXgD18IvvmPIQ++H9a2+cyBmVvOLqz6QVFGVdsuYWRbQ\nFZgX5eFeZrYYWAdc5+5LIweY2UhgJEBb1TWRulIHN3BFztbP7XI4DwzrGr8nUH0gqaKY31A1sybA\ni8Dv3H1rxMMLgXbu3hn4f8DL0a7h7rnunuPuOS1atKhuzCIJ4/n8tXsl9oJJg+Kb2CEuJYQlvcQ0\nczezDEKJPc/dX4p8vHyyd/cZZvaQmR3i7hvjF6pIYolM6peenM0tg4+rnSdTfSCpolh2yxjwOLDc\n3e+tYMxhwNfu7mZ2IqHfCDbFNVKRBHHbK0t48oPwJZI6uRlJ9YGkCmKZufcGLgY+MbNFpX03AW0B\n3P0R4ALgCjPbBXwPDPOg7o4SqUWRs/V7L+zMkG6tA4pGpGKx7JZ5D9jn2/3u/iDwYLyCEkk0Ax+Y\nw7L14W81qXSAJDLVlhHZh5IS54ibZoT1vTyqN13aNAsoIpHYKLmLVCByCQY0W5fkoeQuEuE/O3dx\n/G3/DOubd1NffqJCX5JElNxFytFsXVKFkrsIsPbbIk758zthfTUq9CUSMCV3SXuarUsqUnKXtPXB\nqk38/NEPw/riVuhLJGBK7pKWImfrP23fnL/9pmdA0YjEn5K7pJWnPijg1lfCC5ZqCUZSkZK7pI3I\n2frVpx/Jtf2PDigakdql5C4p7/63VnD/W5+F9Wm2LqlOyV1SWuRsffIvujGoU8uAohGpOzEf1iFS\na/LyICsL6tULfczLq/ElL3syP+ohGkrski40c5dg5eWFH2C9Zk2oDdWqXb67xGkfUehr5rU/44gW\nTWoaqUhSsaDKrufk5Hh+fn4gzy0JJCsr+tmg7dpBQUGVLtV1/Bt8V1Qc1qe1dUk1ZrbA3XMqG6eZ\nuwTriy+q1h/F9p276BhR6Gvxrf05sFFGTSITSWqxHLPXBngKOAwoAXLd/YGIMQY8AAwEioAR7r4w\n/uFKymnbNvrMvW3bmL5cpQNEootl5r4LuNbdF5pZU2CBmb3p7svKjTkL6FD65yTg4dKPIvs2YUL4\nmjtAo0ah/n0o/K6Ik+8KL/T12YSzyKivPQIiENsxe+uB9aWfbzOz5UAroHxyPxd4qvTc1A/NrJmZ\ntSz9WpGK/fim6dixoaWYtm1DiX0fb6ZGztZPzDqY5y7vVZtRiiSdKq25m1kW0BWYF/FQK2BtuXZh\naV9YcjezkcBIgLYx/totaWD48Jh2xixY8y3nP/xBWJ+WYESiizm5m1kT4EXgd+6+NfLhKF+y1zYc\nd88FciG0W6YKcUqai5ytX3ZyNjcPPi6gaEQSX0zJ3cwyCCX2PHd/KcqQQqBNuXZrYF3Nw5N099LC\nQv7w3OKwPs3WRSoXy24ZAx4Hlrv7vRUMmwZcZWbPEnojdYvW26WmImfrf76gExfmtKlgtIiUF8vM\nvTdwMfCJmS0q7bsJaAvg7o8AMwhtg1xJaCvkJfEPVdLFna8t53/f/TysT7N1kaqJZbfMe0RfUy8/\nxoFR8QpK0lfkbP253/bixOyDA4pGJHnpDlVJCL949EPeX7UprE+zdZHqU3KXQO3aXcKRY18L65tz\nw2m0ObhRQBGJpAYldwlMh7EzKN4dviNWs3WR+FBylzq35ftiOv/pjbC+T8b1p2mmCn2JxIuSu9Sp\nyDdMmzRswJI/nRlQNCKpS8ld6sRXW3bQ8863w/pWTRxI/Xr73IglItWkEnrxVAvHxaWCrDHTwxJ7\nn6NbUDBpUGomdn0PSILQzD1e4nxcXCpYum4Lg/7yXlhfSr9hqu8BSSCaucfL2LHhNckh1B47Nph4\nApY1ZnpYYr+rVREFz45K7RmtvgckgWjmHi9xOC4uFby9/GsufTL8bNyCEzanx4xW3wOSQJTc46WG\nx8WlgsidMHmXnUTvIw8JzdQrmtGmUnLX94AkEC3LxMuECaHj4cqL4bi4VPDXuav3SuwFkwaFEjuk\nz4w2jb8HJPEoucfL8OGQmwvt2oFZ6GNubt3NTAPYpeHuZI2Zzp9e3XPi4lt/OHXvN00rmrmm2ow2\n6O8BkXIsVNCx7uXk5Hh+fn7lA6Vykbs0IDRjrMXEcvPLn/D0h+Ez7wp3wgQQn0iqMrMF7p5T6Tgl\n9xSQlRV9rbddOygoiOtTRSv0lX/zGRzSpOG+vzAvr0qHYItIdEru6aRePYj272gGJSVxe5rzH36f\nBWu+K2u3OXh/5txwetyuLyKVizW5x3LM3hPAYOAbd+8Y5fE+wCvA6tKul9x9fNXClRqp5V0a23YU\nc8K48EJfn94+gMyM+nG5vojEXyxbIacADwJP7WPMHHcfHJeIpOomTIi+ph2HXRqRZXnP6ngYD/+y\ne42vKyK1K5Zj9mabWVbthyLV9uPadRzXtAu/K+Lku94J6/t84kDqpWI9GJEUFK+bmHqZ2WJgHXCd\nuy+NNsjMRgIjAdqm2ja4oA0fHrc3KCP3rF/TtwN/6HdUXK4tInUjHsl9IdDO3beb2UDgZaBDtIHu\nngvkQugN1Tg8t8TR4rWbOXfy3LC+lC70JZLCapzc3X1ruc9nmNlDZnaIu2+s6bWl7kTO1u+/qAvn\ndW0VUDQiUlM1Tu5mdhjwtbu7mZ1I6K7XTZV8mSSI15es5/KnF4b1abYukvxi2Qr5DNAHOMTMCoHb\ngAwAd38EuAC4wsx2Ad8DwzyozfNSJZGz9ed+24sTsw8OKBoRiadYdsv8vJLHHyS0VVKSxCPvrmLS\na5+G9Wm2LpJaVPI3jbg72TfOCOt757o+ZB/SOKCIRKS2qCpkVSXpGZnXPrd4r8ReMGmQErtIitLM\nvSqS8IzMH3aVcNTN4YW+Ft3aj2aN9gsoIhGpCyocVhV1WH0xHs56YA7L15ftVOWYw5ry+u9ODTAi\nEampuBUOk3KS5EShLUXFdB4fXujr33cMoGEDFfoSSRfJteYe9Hp3EpwolDVmelhi/6+urSiYNEiJ\nXSTNJM/MPRHWu2ux+mJNfbNtBydOeDusb/WdAzFToS+RdJQ8M/exY8OTKoTaY8fWXQwJekZm3/+Z\nFZbYbxhwNAWTBimxi6Sx5HlDtY5OG0omK7/Zzhn3vhvWp5uRRFJb6r2hWsunDSWbyNIBL17xU7q3\nOyigaEQk0STPssyECaH17fISZL27Ls0v+DYssZuFZutK7CJSXvLM3GvhtKFkEzlbV+kAEalI8iR3\niOtpQ8lk+sfrGfW3PWV5dTOSiFQmuZJ7molW6Cv/5jM4pEnDgCISkWSh5J6gHpvzOXdMX17WHnRC\nSyYP7xZgRCKSTGI5rOMJYDDwjbt3jPK4AQ8AA4EiYIS7L4wcJ7Ep3l1Ch7Hhhb6WjT+TRvvp57CI\nxC6W3TJTgAH7ePwsQgdidwBGAg/XPKz0NG7a0rDEfmWf9hRMGqTELiJVFstJTLPNLGsfQ84Fnio9\nWu9DM2tmZi3dfX2cYkx523YUc8K48EJfqyYOpH493WEqItUTjylhK2BtuXZhaZ+Sewx+/cS/eHfF\nhrL2xP86gV+clJ43ZolI/MQjuUebXkataWBmIwkt3dA2Te8s/dFXW3bQ804V+hKR2hGP5F4ItCnX\nbg2sizbQ3XOBXAjVlonDcyelk++aSeF335e1H/91Dn2P/UmAEYlIqolHcp8GXGVmzwInAVu03h7d\niq+30f++2WF9KvQlIrUhlq2QzwB9gEPMrBC4DcgAcPdHgBmEtkGuJLQV8pLaCjaZRZYOeGVUbzq3\naRZQNCKS6mLZLfPzSh53YFRetDAmAAAHPUlEQVTcIkox76/ayC8enVfWbrxffZaO39fOUhGRmtMG\n6loUOVufff1ptG3eqILRIiLxo+ReC15Z9CWjn11U1u7cphmvjOodYEQikm6U3OMoWqGvj27px0GN\n9wsoIhFJV8lzWEeCe2XRl2GJfUjXVhRMGqTELiKB0My9hqIV+vr3HQNo2KB+QBGJiCi510ju7FVM\nnPFpWfvuCzoxNKfNPr5CRKRuKLlXw3927uL42/4Z1vf5xIHUU6EvEUkQSu5V9MKCQq57fnFZ+6+X\n9OC0ow8NMCIRkb0pucdo645iOpUry7t/Rn2W366bkUQkMSm5xyBybX3WdX3IOqRxgBGJiOybkvs+\nfLNtBydO2FOW99KTs7ll8HEBRiQiEhsl9wpMmL6MR+esLmv/66a+HHpAZoARiYjETsk9wppN/+Fn\nd88qa/9xwDFc0ad9cAGJiFSDkns5o5/9iFcW7TlnZPFt/Tlw/4wAIxIRqR4ld2Dpui0M+st7Ze0/\nX9CJC3UzkogksbRO7u7OsNwPmbf6WwCaZjZg/tgzyMxQ6QARSW4xFQ4zswFm9m8zW2lmY6I8PsLM\nNpjZotI/l8U/1Pj68PNNZN84oyyxP/qrHD4Zd6YSu4ikhFiO2asPTAb6EToMe76ZTXP3ZRFD/+7u\nV9VCjHG1a3cJ/e6bzeqN/wHgyEOb8ProU2hQXwUyRSR1xLIscyKw0t0/Byg9CPtcIDK5J7zXl3zF\n5U8vKGs/99tenJh9cIARiYjUjliSeytgbbl2IXBSlHHnm9mpwArg9+6+NnKAmY0ERgK0bdu26tFW\n047i3XS7/U2KftgNQO8jm/P0pSdhpkJfIpKaYlmLiJYBPaL9KpDl7p2At4Ano13I3XPdPcfdc1q0\naFG1SKvp7/O/4JhbXi9L7K+NPoW8y3oqsYtISotl5l4IlN8X2BpYV36Au28q13wUuKvmodXMlqJi\nOo/fU+hrSLdW3HthlwAjEhGpO7Ek9/lABzPLBr4EhgG/KD/AzFq6+/rS5jnA8rhGWUWT31nJ3f/8\nd1l7zg2n0ebgRgFGJCJStypN7u6+y8yuAv4J1AeecPelZjYeyHf3acA1ZnYOsAv4FhhRizFX6Out\nOzhp4p5CX5f/rD1jzjomiFBERAJl7pHL53UjJyfH8/Pz43a9cdOWMuX9grL2/LFn0KJpw7hdX0Qk\nEZjZAnfPqWxc0t+hunrjfzjtnlll7ZsHHctlpxwRXEAiIgkgaZO7u3PV3z5i+ifry/o+Gdefppkq\n9CUikpTJ/ZPCLZz94J5CX/de2Jkh3VoHGJGISGJJuuS+9tuissTevPF+zB1zuurBiIhESLrk3qRh\nA3of2ZxLT87m9GN+EnQ4IiIJKemS+0GN9yPvsp5BhyEiktBUClFEJAUpuYuIpCAldxGRFKTkLiKS\ngpTcRURSkJK7iEgKUnIXEUlBSu4iIikosJK/ZrYBWBPD0EOAjbUcTjLS61IxvTbR6XWpWDK9Nu3c\nvdJzSgNL7rEys/xYahenG70uFdNrE51el4ql4mujZRkRkRSk5C4ikoKSIbnnBh1AgtLrUjG9NtHp\ndalYyr02Cb/mLiIiVZcMM3cREakiJXcRkRSUkMndzNqY2TtmttzMlprZ6KBjSiRmVt/MPjKzfwQd\nSyIxs2Zm9oKZfVr6vdMr6JgShZn9vvT/0hIze8bMMoOOKShm9oSZfWNmS8r1HWxmb5rZZ6UfDwoy\nxnhIyOQO7AKudfdjgZ7AKDM7LuCYEsloYHnQQSSgB4DX3f0YoDN6jQAws1bANUCOu3cE6gPDgo0q\nUFOAARF9Y4C33b0D8HZpO6klZHJ39/XuvrD0822E/pO2CjaqxGBmrYFBwGNBx5JIzOwA4FTgcQB3\n/8HdNwcbVUJpAOxvZg2ARsC6gOMJjLvPBr6N6D4XeLL08yeB8+o0qFqQkMm9PDPLAroC84KNJGHc\nD9wAlAQdSII5AtgA/LV0yeoxM2scdFCJwN2/BO4BvgDWA1vc/Y1go0o4P3H39RCaXAKHBhxPjSV0\ncjezJsCLwO/cfWvQ8QTNzAYD37j7gqBjSUANgG7Aw+7eFfgPKfCrdTyUrh+fC2QDhwONzeyXwUYl\ntS1hk7uZZRBK7Hnu/lLQ8SSI3sA5ZlYAPAucbmZPBxtSwigECt39x9/wXiCU7AXOAFa7+wZ3LwZe\nAn4acEyJ5mszawlQ+vGbgOOpsYRM7mZmhNZOl7v7vUHHkyjc/UZ3b+3uWYTeEJvp7pqBAe7+FbDW\nzI4u7eoLLAswpETyBdDTzBqV/t/qi95sjjQN+HXp578GXgkwlrhoEHQAFegNXAx8YmaLSvtucvcZ\nAcYkie9qIM/M9gM+By4JOJ6E4O7zzOwFYCGhnWgfkYK328fKzJ4B+gCHmFkhcBswCXjOzC4l9MNw\naHARxofKD4iIpKCEXJYREZGaUXIXEUlBSu4iIilIyV1EJAUpuYuIpCAldxGRFKTkLiKSgv4/177t\ntaGcbFUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c70fa90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the graph\n",
    "predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "plt.plot(x_train, y_train, 'ro', label='Original data')\n",
    "plt.plot(x_train, predicted, label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST Dataset (Images and Labels)\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# Dataset Loader (Input Pipline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n",
      "<class 'torch.FloatTensor'> <class 'torch.LongTensor'>\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "image, label = iter(train_loader).next()\n",
    "print(type(image), type(label))\n",
    "print(image.size(), label.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [100/600], Loss: 2.1991\n",
      "Epoch: [1/5], Step: [200/600], Loss: 2.1090\n",
      "Epoch: [1/5], Step: [300/600], Loss: 2.0185\n",
      "Epoch: [1/5], Step: [400/600], Loss: 1.8627\n",
      "Epoch: [1/5], Step: [500/600], Loss: 1.8895\n",
      "Epoch: [1/5], Step: [600/600], Loss: 1.8074\n",
      "Epoch: [2/5], Step: [100/600], Loss: 1.7375\n",
      "Epoch: [2/5], Step: [200/600], Loss: 1.6430\n",
      "Epoch: [2/5], Step: [300/600], Loss: 1.6553\n",
      "Epoch: [2/5], Step: [400/600], Loss: 1.5521\n",
      "Epoch: [2/5], Step: [500/600], Loss: 1.6084\n",
      "Epoch: [2/5], Step: [600/600], Loss: 1.4299\n",
      "Epoch: [3/5], Step: [100/600], Loss: 1.3624\n",
      "Epoch: [3/5], Step: [200/600], Loss: 1.3872\n",
      "Epoch: [3/5], Step: [300/600], Loss: 1.2938\n",
      "Epoch: [3/5], Step: [400/600], Loss: 1.3767\n",
      "Epoch: [3/5], Step: [500/600], Loss: 1.2836\n",
      "Epoch: [3/5], Step: [600/600], Loss: 1.2734\n",
      "Epoch: [4/5], Step: [100/600], Loss: 1.2175\n",
      "Epoch: [4/5], Step: [200/600], Loss: 1.1995\n",
      "Epoch: [4/5], Step: [300/600], Loss: 1.1280\n",
      "Epoch: [4/5], Step: [400/600], Loss: 1.1507\n",
      "Epoch: [4/5], Step: [500/600], Loss: 1.2373\n",
      "Epoch: [4/5], Step: [600/600], Loss: 1.0780\n",
      "Epoch: [5/5], Step: [100/600], Loss: 1.0654\n",
      "Epoch: [5/5], Step: [200/600], Loss: 1.0851\n",
      "Epoch: [5/5], Step: [300/600], Loss: 1.0039\n",
      "Epoch: [5/5], Step: [400/600], Loss: 1.0853\n",
      "Epoch: [5/5], Step: [500/600], Loss: 1.0733\n",
      "Epoch: [5/5], Step: [600/600], Loss: 0.9628\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # ここではlogitsを返してsoftmaxを通さないので注意\n",
    "        # LinearRegressionとまったく同じ\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "model = LogisticRegression(input_size, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28 * 28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 82 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28 * 28))\n",
    "    outputs = model(images)\n",
    "    # torch.maxは最大値とそのインデックスの両方を返す\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    # predicted == labelsはByteTensor\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST Dataset \n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),  \n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "net = Net(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.3688\n",
      "Epoch [1/5], Step [200/600], Loss: 0.2342\n",
      "Epoch [1/5], Step [300/600], Loss: 0.1315\n",
      "Epoch [1/5], Step [400/600], Loss: 0.1842\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1777\n",
      "Epoch [1/5], Step [600/600], Loss: 0.1968\n",
      "Epoch [2/5], Step [100/600], Loss: 0.1537\n",
      "Epoch [2/5], Step [200/600], Loss: 0.1552\n",
      "Epoch [2/5], Step [300/600], Loss: 0.1092\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0898\n",
      "Epoch [2/5], Step [500/600], Loss: 0.1120\n",
      "Epoch [2/5], Step [600/600], Loss: 0.2084\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0941\n",
      "Epoch [3/5], Step [200/600], Loss: 0.1204\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0847\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0379\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0408\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0791\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0432\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0607\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0792\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0530\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0659\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0372\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0121\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0441\n",
      "Epoch [5/5], Step [300/600], Loss: 0.1009\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0522\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0863\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0393\n"
     ]
    }
   ],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Convert torch tensor to Variable\n",
    "        images = Variable(images.view(-1, 28 * 28))\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                   %(epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28 * 28))\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "# Save the Model\n",
    "torch.save(net.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Iter [100/600] Loss: 0.1731\n",
      "Epoch [1/5], Iter [200/600] Loss: 0.1133\n",
      "Epoch [1/5], Iter [300/600] Loss: 0.0820\n",
      "Epoch [1/5], Iter [400/600] Loss: 0.0718\n",
      "Epoch [1/5], Iter [500/600] Loss: 0.0273\n",
      "Epoch [1/5], Iter [600/600] Loss: 0.0293\n",
      "Epoch [2/5], Iter [100/600] Loss: 0.0405\n",
      "Epoch [2/5], Iter [200/600] Loss: 0.0985\n",
      "Epoch [2/5], Iter [300/600] Loss: 0.0085\n",
      "Epoch [2/5], Iter [400/600] Loss: 0.0209\n",
      "Epoch [2/5], Iter [500/600] Loss: 0.0563\n",
      "Epoch [2/5], Iter [600/600] Loss: 0.0210\n",
      "Epoch [3/5], Iter [100/600] Loss: 0.0162\n",
      "Epoch [3/5], Iter [200/600] Loss: 0.0455\n",
      "Epoch [3/5], Iter [300/600] Loss: 0.0174\n",
      "Epoch [3/5], Iter [400/600] Loss: 0.0038\n",
      "Epoch [3/5], Iter [500/600] Loss: 0.0182\n",
      "Epoch [3/5], Iter [600/600] Loss: 0.0953\n",
      "Epoch [4/5], Iter [100/600] Loss: 0.0089\n",
      "Epoch [4/5], Iter [200/600] Loss: 0.0121\n",
      "Epoch [4/5], Iter [300/600] Loss: 0.0560\n",
      "Epoch [4/5], Iter [400/600] Loss: 0.0060\n",
      "Epoch [4/5], Iter [500/600] Loss: 0.0140\n",
      "Epoch [4/5], Iter [600/600] Loss: 0.0047\n",
      "Epoch [5/5], Iter [100/600] Loss: 0.0033\n",
      "Epoch [5/5], Iter [200/600] Loss: 0.0141\n",
      "Epoch [5/5], Iter [300/600] Loss: 0.0087\n",
      "Epoch [5/5], Iter [400/600] Loss: 0.0155\n",
      "Epoch [5/5], Iter [500/600] Loss: 0.0049\n",
      "Epoch [5/5], Iter [600/600] Loss: 0.0286\n",
      "Test Accuracy of the model on the 10000 test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Hyper Parameters\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = dsets.MNIST(root='./data/',\n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data/',\n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# CNN Model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # [Conv > BN > ReLU > Pool] のブロックをまとめられる\n",
    "        # in:[N, 1, 28, 28] out:[N, 16, 14, 14]\n",
    "        self.layer1 = nn.Sequential(\n",
    "            # PyTorchではin_channel, out_channelの両方とも明示する必要あり\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            # PyTorchではBatchNormも入力のfeatures数を明示する必要あり\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        # in:[N, 16, 14, 14] out:[N, 32, 7, 7]\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        # in:[N, 7*7*32] out:[N, 10]\n",
    "        self.fc = nn.Linear(7 * 7 * 32, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         print('in:', x.size())\n",
    "        out = self.layer1(x)\n",
    "#         print('out layer1:', out.size())\n",
    "        out = self.layer2(out)\n",
    "#         print('out layer2:', out.size())\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "#         print('out fc:', out.size())\n",
    "        return out\n",
    "\n",
    "cnn = CNN()\n",
    "\n",
    "# モデルの各層のテンソルサイズを見たいときはダミーの入力を渡すと簡単\n",
    "# images, labels = iter(test_loader).next()\n",
    "# cnn(Variable(images))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n",
    "                   %(epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.data[0]))\n",
    "\n",
    "# Test the Model\n",
    "# BNを使っているときはモデルをevalモードにする\n",
    "cnn.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images)\n",
    "    outputs = cnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "# Save the Trained Model\n",
    "torch.save(cnn.state_dict(), 'cnn.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Image Processing\n",
    "# Data Augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Scale(40),  # 少し大きめに拡張してランダムクロップする\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "# CIFAR-10 dataset\n",
    "train_dataset = dsets.CIFAR10(root='./data/',\n",
    "                              train=True,\n",
    "                              transform=transform,\n",
    "                              download=True)\n",
    "\n",
    "# テストデータは Data Augmentation しない\n",
    "test_dataset = dsets.CIFAR10(root='./data/',\n",
    "                             train=False,\n",
    "                             transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=100,\n",
    "                                          shuffle=False)\n",
    "\n",
    "# 3x3 convolution\n",
    "def conv3x3(in_channels, out_channels, stride=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
    "                     stride=stride, padding=1, bias=False)\n",
    "\n",
    "# Residual Block\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(out_channels, out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        # 入力のresidualを足し込めるようにサイズやチャネル数を調整するdownsampleを入れる\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# ResNet Module\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers=[2, 2, 2], num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_channels = 16\n",
    "        self.conv = conv3x3(3, 16)\n",
    "        self.bn = nn.BatchNorm2d(16)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.layer1 = self.make_layer(block, 16, layers[0])\n",
    "        self.layer2 = self.make_layer(block, 32, layers[1], stride=2)\n",
    "        self.layer3 = self.make_layer(block, 64, layers[2], stride=2)\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        # チャネル数が変化するタイミングでdownsampleを入れる\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                conv3x3(self.in_channels, out_channels, stride=stride),\n",
    "                nn.BatchNorm2d(out_channels))\n",
    "        layers = []\n",
    "        layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
    "        # 残りはin_channels == out_channelsなのでdownsampleは入らない\n",
    "        self.in_channels = out_channels\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "            \n",
    "    def forward(self, x):\n",
    "#         print('x:', x.size())       # (N, 3, 32, 32)\n",
    "        out = self.conv(x)  \n",
    "#         print('out1:', out.size())  # (N, 16, 32, 32)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer1(out)\n",
    "#         print('out2:', out.size())  # (N, 16, 32, 32)\n",
    "        out = self.layer2(out)      # stride=2なのでサイズは半分になる\n",
    "#         print('out3:', out.size())  # (N, 32, 16, 16)\n",
    "        out = self.layer3(out)      # stride=2なのでサイズは半分になる\n",
    "#         print('out4:', out.size())  # (N, 64, 8, 8)\n",
    "        out = self.avg_pool(out)\n",
    "#         print('out5:', out.size())  # (N, 64, 1, 1)\n",
    "        out = out.view(out.size(0), -1)\n",
    "#         print('out6:', out.size())  # (N, 64)\n",
    "        out = self.fc(out)\n",
    "#         print('out7:', out.size())  # (N, 10)\n",
    "        return out\n",
    "\n",
    "resnet = ResNet(ResidualBlock, [2, 2, 2])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 0.001\n",
    "optimizer = torch.optim.Adam(resnet.parameters(), lr=lr)\n",
    "\n",
    "# training\n",
    "for epoch in range(80):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print (\"Epoch [%d/%d], Iter [%d/%d] Loss: %.4f\" %(epoch + 1, 80, i + 1, 500, loss.data[0]))\n",
    "\n",
    "    # decaying learning rate\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        lr /= 3\n",
    "        optimizer = torch.optim.Adam(resnet.parameters(), lr=lr)\n",
    "\n",
    "# Test\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images)\n",
    "    outputs = resnet(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the model on the test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "# Save the Model\n",
    "torch.save(resnet.state_dict(), 'resnet.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# resnet = ResNet(ResidualBlock, [2, 2, 2])\n",
    "# images, labels = iter(test_loader).next()\n",
    "# out = resnet(Variable(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet (\n",
       "  (conv): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (relu): ReLU (inplace)\n",
       "  (layer1): Sequential (\n",
       "    (0): ResidualBlock (\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (1): ResidualBlock (\n",
       "      (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential (\n",
       "    (0): ResidualBlock (\n",
       "      (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential (\n",
       "        (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock (\n",
       "      (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential (\n",
       "    (0): ResidualBlock (\n",
       "      (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential (\n",
       "        (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock (\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (avg_pool): AvgPool2d (size=8, stride=8, padding=0, ceil_mode=False, count_include_pad=True)\n",
       "  (fc): Linear (64 -> 10)\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/600], Loss: 0.7648\n",
      "Epoch [1/2], Step [200/600], Loss: 0.2547\n",
      "Epoch [1/2], Step [300/600], Loss: 0.1060\n",
      "Epoch [1/2], Step [400/600], Loss: 0.0535\n",
      "Epoch [1/2], Step [500/600], Loss: 0.1770\n",
      "Epoch [1/2], Step [600/600], Loss: 0.1920\n",
      "Epoch [2/2], Step [100/600], Loss: 0.0510\n",
      "Epoch [2/2], Step [200/600], Loss: 0.1051\n",
      "Epoch [2/2], Step [300/600], Loss: 0.1339\n",
      "Epoch [2/2], Step [400/600], Loss: 0.0542\n",
      "Epoch [2/2], Step [500/600], Loss: 0.0500\n",
      "Epoch [2/2], Step [600/600], Loss: 0.0963\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Hyper parameters\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "num_epochs = 2\n",
    "learning_rate = 0.01\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = dsets.MNIST(root='./data/',\n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data/',\n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# RNN Model (Many-to-One)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        # num_layersが1より大きい場合はStacked LSTMになる\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, input_size)\n",
    "        # 入力はinput_sizeの特徴ベクトルのシーケンス\n",
    "\n",
    "        # set initial states\n",
    "        # バッチ内のサンプルごとに状態を別々に管理している\n",
    "        h0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "        c0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size))\n",
    "#         print('h0:', h0.size())\n",
    "#         print('c0:', c0.size())\n",
    "\n",
    "        # forward propagate RNN\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "#         print('out1:', out.size())\n",
    "\n",
    "        # decode hidden state of last time step\n",
    "        # 入力の各シーケンスごとに出力されている\n",
    "        # many-to-oneの場合は最後の出力を使う\n",
    "        # out: (batch, seq_len, hidden_size)\n",
    "        out = self.fc(out[:, -1, :])        \n",
    "#         print('out2:', out.size())\n",
    "\n",
    "        return out\n",
    "\n",
    "rnn = RNN(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, sequence_length, input_size))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = rnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                   %(epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 28, 28])\n",
      "h0: torch.Size([2, 100, 128])\n",
      "c0: torch.Size([2, 100, 128])\n",
      "out1: torch.Size([100, 28, 128])\n",
      "out2: torch.Size([100, 10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.0506  0.0622  0.0198  ...  -0.0239 -0.0965 -0.0603\n",
       " 0.0492  0.0620  0.0196  ...  -0.0211 -0.0951 -0.0617\n",
       " 0.0524  0.0628  0.0167  ...  -0.0242 -0.1008 -0.0589\n",
       "          ...             ⋱             ...          \n",
       " 0.0528  0.0645  0.0128  ...  -0.0219 -0.1035 -0.0592\n",
       " 0.0513  0.0617  0.0169  ...  -0.0242 -0.0984 -0.0606\n",
       " 0.0522  0.0627  0.0167  ...  -0.0234 -0.1011 -0.0591\n",
       "[torch.FloatTensor of size 100x10]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "images, labels = iter(train_loader).next()\n",
    "print(images.size(), labels.size())\n",
    "# 28x28の画像を28次元ベクトルの長さ28の系列とみなす\n",
    "images = images.view(-1, 28, 28)\n",
    "print(images.size())\n",
    "rnn(Variable(images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 96 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, sequence_length, input_size))\n",
    "    outputs = rnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total)) \n",
    "\n",
    "# Save the Model\n",
    "torch.save(rnn.state_dict(), 'rnn.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/600], Loss: 0.7885\n",
      "Epoch [1/2], Step [200/600], Loss: 0.2718\n",
      "Epoch [1/2], Step [300/600], Loss: 0.2205\n",
      "Epoch [1/2], Step [400/600], Loss: 0.3021\n",
      "Epoch [1/2], Step [500/600], Loss: 0.1091\n",
      "Epoch [1/2], Step [600/600], Loss: 0.1574\n",
      "Epoch [2/2], Step [100/600], Loss: 0.1384\n",
      "Epoch [2/2], Step [200/600], Loss: 0.0712\n",
      "Epoch [2/2], Step [300/600], Loss: 0.0799\n",
      "Epoch [2/2], Step [400/600], Loss: 0.0320\n",
      "Epoch [2/2], Step [500/600], Loss: 0.0132\n",
      "Epoch [2/2], Step [600/600], Loss: 0.1111\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Hyper Parameters\n",
    "sequence_length = 28\n",
    "input_size = 28\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "num_epochs = 2\n",
    "learning_rate = 0.003\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = dsets.MNIST(root='./data/',\n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data/',\n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True, bidirectional=True)\n",
    "        # bidirectionalのときは出力が順方向と逆方向で2倍になる\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # set initial states\n",
    "        h0 = Variable(torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size))\n",
    "        c0 = Variable(torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size))\n",
    "        \n",
    "        # forward propagate RNN\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # decode hidden state of last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "rnn = BiRNN(input_size, hidden_size, num_layers, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, sequence_length, input_size))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = rnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                   %(epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BiRNN(\n",
       "  (lstm): LSTM(28, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=256, out_features=10)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 97 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, sequence_length, input_size))\n",
    "    outputs = rnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total)) \n",
    "\n",
    "# Save the Model\n",
    "torch.save(rnn.state_dict(), 'rnn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
