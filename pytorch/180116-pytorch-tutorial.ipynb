{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial\n",
    "https://github.com/yunjey/pytorch-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自動微分 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 2\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 1\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create tensors\n",
    "# Variableは自動微分の対象になる\n",
    "x = Variable(torch.Tensor([1]), requires_grad=True)\n",
    "w = Variable(torch.Tensor([2]), requires_grad=True)\n",
    "b = Variable(torch.Tensor([3]), requires_grad=True)\n",
    "\n",
    "# build a computational graph\n",
    "y = w * x + b  # y = 2 * x + 3\n",
    "\n",
    "# compute gradients\n",
    "y.backward()\n",
    "\n",
    "# print gradients\n",
    "print(x.grad)  # dy/dx = w\n",
    "print(w.grad)  # dy/dw = x\n",
    "print(b.grad)  # dy/db = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自動微分 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: Parameter containing:\n",
      "-0.0213  0.3410  0.4479\n",
      "-0.2651  0.4823 -0.2477\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "b: Parameter containing:\n",
      " 0.2191\n",
      " 0.0330\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "loss: 0.42183035612106323\n",
      "dL/dw: Variable containing:\n",
      " 0.2508  0.3263 -0.0476\n",
      " 0.4193  0.6737 -0.2881\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "dL/db: Variable containing:\n",
      "-0.0772\n",
      "-0.1106\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "*** by hand\n",
      "\n",
      "-0.0238  0.3377  0.4484\n",
      "-0.2693  0.4755 -0.2448\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "\n",
      " 0.2199\n",
      " 0.0341\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "*** by step()\n",
      "Parameter containing:\n",
      "-0.0238  0.3377  0.4484\n",
      "-0.2693  0.4755 -0.2448\n",
      "[torch.FloatTensor of size 2x3]\n",
      "\n",
      "Parameter containing:\n",
      " 0.2199\n",
      " 0.0341\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# autograd 2\n",
    "x = Variable(torch.randn(5, 3))\n",
    "y = Variable(torch.randn(5, 2))\n",
    "\n",
    "# linear layer\n",
    "linear = nn.Linear(3, 2)\n",
    "print('w:', linear.weight)\n",
    "print('b:', linear.bias)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(linear.parameters(), lr=0.01)\n",
    "\n",
    "# forward\n",
    "pred = linear(x)\n",
    "\n",
    "# compute loss\n",
    "loss = criterion(pred, y)\n",
    "print('loss:', loss.data[0])\n",
    "\n",
    "# backpropagation\n",
    "loss.backward()\n",
    "\n",
    "# print out the gradients\n",
    "print('dL/dw:', linear.weight.grad)\n",
    "print('dL/db:', linear.bias.grad)\n",
    "\n",
    "print('*** by hand')\n",
    "# optimizer.step()の中身は下の計算をしている\n",
    "# 計算結果が一致する\n",
    "print(linear.weight.data.sub(0.01 * linear.weight.grad.data))\n",
    "print(linear.bias.data.sub(0.01 * linear.bias.grad.data))\n",
    "\n",
    "# gradient descent\n",
    "optimizer.step()\n",
    "\n",
    "print('*** by step()')\n",
    "print(linear.weight)\n",
    "print(linear.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ndarray <=> tensor の相互変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'torch.LongTensor'>\n",
      "<class 'numpy.ndarray'>\n",
      "[[1 2]\n",
      " [3 4]]\n",
      "\n",
      " 1  2\n",
      " 3  4\n",
      "[torch.LongTensor of size 2x2]\n",
      "\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Loading data from numpy\n",
    "a = np.array([[1, 2], [3, 4]])\n",
    "b = torch.from_numpy(a)  # ndarray => tensor\n",
    "c = b.numpy()            # tensor  => ndarray\n",
    "print(type(a))\n",
    "print(type(b))\n",
    "print(type(c))\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DatasetとDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "torch.Size([3, 32, 32])\n",
      "6\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "# Implementing the input pipeline\n",
    "# Download and construct dataset\n",
    "# Datasetクラスのサブクラス（__getitem__と__len__が実装されている）\n",
    "train_dataset = dsets.CIFAR10(root='./data',\n",
    "                              train=True,\n",
    "                              transform=transforms.ToTensor(),\n",
    "                              download=True)\n",
    "\n",
    "# select one data pair\n",
    "# ディスクからデータを読む\n",
    "image, label = train_dataset[0]\n",
    "print(image.size())\n",
    "print(label)\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 32, 32]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# data loader\n",
    "# Datasetを渡すとバッチ単位でファイルからデータを取り出せる\n",
    "# num_workersを指定することでマルチプロセッサで並列に読み出せる？\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=100,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=2)\n",
    "data_iter = iter(train_loader)\n",
    "\n",
    "# ミニバッチ単位で画像とラベルをロード\n",
    "images, labels = data_iter.next()\n",
    "print(images.size(), labels.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 3, 32, 32]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# 実際はiterを使わなくてforループでOK\n",
    "for images, labels in train_loader:\n",
    "    print(images.size(), labels.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カスタムデータセット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `__getitem__()` と `__len__()` を定義する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CustomDataset(data.Dataset):\n",
    "    def __init__(self):\n",
    "        # initialize file path or list of file names.\n",
    "        pass\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 1. Read one data from file (e.g. using numpy.fromfile, PIL.Image.open)\n",
    "        # 2. Preprocess the data (e.g. torchvision.Transform)\n",
    "        # 3. Return a data pair (e.g. image and label)\n",
    "        pass\n",
    "\n",
    "    def __len__(self):\n",
    "        # return total size of your dataset\n",
    "        return 0\n",
    "\n",
    "custom_dataset = CustomDataset()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=custom_dataset,\n",
    "                                           batch_size=100,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習済みモデルを使う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download and load pretrained resnet\n",
    "resnet = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet (\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "  (relu): ReLU (inplace)\n",
       "  (maxpool): MaxPool2d (size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1))\n",
       "  (layer1): Sequential (\n",
       "    (0): BasicBlock (\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (1): BasicBlock (\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential (\n",
       "    (0): BasicBlock (\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential (\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock (\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential (\n",
       "    (0): BasicBlock (\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential (\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock (\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential (\n",
       "    (0): BasicBlock (\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (downsample): Sequential (\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock (\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (relu): ReLU (inplace)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d (size=7, stride=7, padding=0, ceil_mode=False, count_include_pad=True)\n",
       "  (fc): Linear (512 -> 1000)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finetuneしたいときはパラメータを固定する\n",
    "for param in resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# topのfc層を置き換える\n",
    "resnet.fc = nn.Linear(resnet.fc.in_features, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# 新しく追加したfc層のパラメータは更新対象\n",
    "for param in resnet.fc.parameters():\n",
    "    print(param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100])\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "images = Variable(torch.randn(10, 3, 256, 256))\n",
    "outputs = resnet(images)\n",
    "print(outputs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    }
   ],
   "source": [
    "# 74番目のクラスに分類された\n",
    "print(np.argmax(outputs[0].data.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルのセーブとロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet (\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (relu): ReLU (inplace)\n",
      "  (maxpool): MaxPool2d (size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1))\n",
      "  (layer1): Sequential (\n",
      "    (0): BasicBlock (\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "    (1): BasicBlock (\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential (\n",
      "    (0): BasicBlock (\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (downsample): Sequential (\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock (\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential (\n",
      "    (0): BasicBlock (\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (downsample): Sequential (\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock (\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential (\n",
      "    (0): BasicBlock (\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (downsample): Sequential (\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock (\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu): ReLU (inplace)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AvgPool2d (size=7, stride=7, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "  (fc): Linear (512 -> 100)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# save and load the entire model\n",
    "torch.save(resnet, 'model.pkl')\n",
    "model = torch.load('model.pkl')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save and load only the model parameters\n",
    "# こちらのほうが推薦されている\n",
    "# ファイルサイズは変わらない？\n",
    "torch.save(resnet.state_dict(), 'params.pkl')\n",
    "resnet.load_state_dict(torch.load('params.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/60], Loss: 1.1912\n",
      "Epoch [20/60], Loss: 0.5840\n",
      "Epoch [30/60], Loss: 0.4831\n",
      "Epoch [40/60], Loss: 0.4653\n",
      "Epoch [50/60], Loss: 0.4611\n",
      "Epoch [60/60], Loss: 0.4592\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# hyper parameters\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "num_epochs = 60\n",
    "learning_rate = 0.001\n",
    "\n",
    "# toy dataset\n",
    "# 15 samples, 1 features\n",
    "x_train = np.array([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], \n",
    "                    [9.779], [6.182], [7.59], [2.167], [7.042], \n",
    "                    [10.791], [5.313], [7.997], [3.1]], dtype=np.float32)\n",
    "\n",
    "y_train = np.array([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], \n",
    "                    [3.366], [2.596], [2.53], [1.221], [2.827], \n",
    "                    [3.465], [1.65], [2.904], [1.3]], dtype=np.float32)\n",
    "\n",
    "# linear regression model\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "model = LinearRegression(input_size, output_size)\n",
    "\n",
    "# loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # convert numpy array to torch Variable\n",
    "    # ndarray => Tensor => Variable\n",
    "    inputs = Variable(torch.from_numpy(x_train))\n",
    "    targets = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print('Epoch [%d/%d], Loss: %.4f' % (epoch + 1, num_epochs, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPLi2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111988a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the graph\n",
    "predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "plt.plot(x_train, y_train, 'ro', label='Original data')\n",
    "plt.plot(x_train, predicted, label='Fitted line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST Dataset (Images and Labels)\n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# Dataset Loader (Input Pipline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n",
      "<class 'torch.FloatTensor'> <class 'torch.LongTensor'>\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "image, label = iter(train_loader).next()\n",
    "print(type(image), type(label))\n",
    "print(image.size(), label.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/5], Step: [100/600], Loss: 2.1991\n",
      "Epoch: [1/5], Step: [200/600], Loss: 2.1090\n",
      "Epoch: [1/5], Step: [300/600], Loss: 2.0185\n",
      "Epoch: [1/5], Step: [400/600], Loss: 1.8627\n",
      "Epoch: [1/5], Step: [500/600], Loss: 1.8895\n",
      "Epoch: [1/5], Step: [600/600], Loss: 1.8074\n",
      "Epoch: [2/5], Step: [100/600], Loss: 1.7375\n",
      "Epoch: [2/5], Step: [200/600], Loss: 1.6430\n",
      "Epoch: [2/5], Step: [300/600], Loss: 1.6553\n",
      "Epoch: [2/5], Step: [400/600], Loss: 1.5521\n",
      "Epoch: [2/5], Step: [500/600], Loss: 1.6084\n",
      "Epoch: [2/5], Step: [600/600], Loss: 1.4299\n",
      "Epoch: [3/5], Step: [100/600], Loss: 1.3624\n",
      "Epoch: [3/5], Step: [200/600], Loss: 1.3872\n",
      "Epoch: [3/5], Step: [300/600], Loss: 1.2938\n",
      "Epoch: [3/5], Step: [400/600], Loss: 1.3767\n",
      "Epoch: [3/5], Step: [500/600], Loss: 1.2836\n",
      "Epoch: [3/5], Step: [600/600], Loss: 1.2734\n",
      "Epoch: [4/5], Step: [100/600], Loss: 1.2175\n",
      "Epoch: [4/5], Step: [200/600], Loss: 1.1995\n",
      "Epoch: [4/5], Step: [300/600], Loss: 1.1280\n",
      "Epoch: [4/5], Step: [400/600], Loss: 1.1507\n",
      "Epoch: [4/5], Step: [500/600], Loss: 1.2373\n",
      "Epoch: [4/5], Step: [600/600], Loss: 1.0780\n",
      "Epoch: [5/5], Step: [100/600], Loss: 1.0654\n",
      "Epoch: [5/5], Step: [200/600], Loss: 1.0851\n",
      "Epoch: [5/5], Step: [300/600], Loss: 1.0039\n",
      "Epoch: [5/5], Step: [400/600], Loss: 1.0853\n",
      "Epoch: [5/5], Step: [500/600], Loss: 1.0733\n",
      "Epoch: [5/5], Step: [600/600], Loss: 0.9628\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # ここではlogitsを返してsoftmaxを通さないので注意\n",
    "        # LinearRegressionとまったく同じ\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "\n",
    "model = LogisticRegression(input_size, num_classes)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images.view(-1, 28 * 28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch: [%d/%d], Step: [%d/%d], Loss: %.4f'\n",
    "                  % (epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the 10000 test images: 82 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28 * 28))\n",
    "    outputs = model(images)\n",
    "    # torch.maxは最大値とそのインデックスの両方を返す\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    # predicted == labelsはByteTensor\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# Hyper Parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST Dataset \n",
    "train_dataset = dsets.MNIST(root='./data', \n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),  \n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data', \n",
    "                           train=False,\n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "net = Net(input_size, hidden_size, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.3688\n",
      "Epoch [1/5], Step [200/600], Loss: 0.2342\n",
      "Epoch [1/5], Step [300/600], Loss: 0.1315\n",
      "Epoch [1/5], Step [400/600], Loss: 0.1842\n",
      "Epoch [1/5], Step [500/600], Loss: 0.1777\n",
      "Epoch [1/5], Step [600/600], Loss: 0.1968\n",
      "Epoch [2/5], Step [100/600], Loss: 0.1537\n",
      "Epoch [2/5], Step [200/600], Loss: 0.1552\n",
      "Epoch [2/5], Step [300/600], Loss: 0.1092\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0898\n",
      "Epoch [2/5], Step [500/600], Loss: 0.1120\n",
      "Epoch [2/5], Step [600/600], Loss: 0.2084\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0941\n",
      "Epoch [3/5], Step [200/600], Loss: 0.1204\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0847\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0379\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0408\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0791\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0432\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0607\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0792\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0530\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0659\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0372\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0121\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0441\n",
      "Epoch [5/5], Step [300/600], Loss: 0.1009\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0522\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0863\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0393\n"
     ]
    }
   ],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n",
    "\n",
    "# Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Convert torch tensor to Variable\n",
    "        images = Variable(images.view(-1, 28 * 28))\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs = net(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Step [%d/%d], Loss: %.4f' \n",
    "                   %(epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 97 %\n"
     ]
    }
   ],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images.view(-1, 28 * 28))\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "# Save the Model\n",
    "torch.save(net.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Iter [100/600] Loss: 0.1731\n",
      "Epoch [1/5], Iter [200/600] Loss: 0.1133\n",
      "Epoch [1/5], Iter [300/600] Loss: 0.0820\n",
      "Epoch [1/5], Iter [400/600] Loss: 0.0718\n",
      "Epoch [1/5], Iter [500/600] Loss: 0.0273\n",
      "Epoch [1/5], Iter [600/600] Loss: 0.0293\n",
      "Epoch [2/5], Iter [100/600] Loss: 0.0405\n",
      "Epoch [2/5], Iter [200/600] Loss: 0.0985\n",
      "Epoch [2/5], Iter [300/600] Loss: 0.0085\n",
      "Epoch [2/5], Iter [400/600] Loss: 0.0209\n",
      "Epoch [2/5], Iter [500/600] Loss: 0.0563\n",
      "Epoch [2/5], Iter [600/600] Loss: 0.0210\n",
      "Epoch [3/5], Iter [100/600] Loss: 0.0162\n",
      "Epoch [3/5], Iter [200/600] Loss: 0.0455\n",
      "Epoch [3/5], Iter [300/600] Loss: 0.0174\n",
      "Epoch [3/5], Iter [400/600] Loss: 0.0038\n",
      "Epoch [3/5], Iter [500/600] Loss: 0.0182\n",
      "Epoch [3/5], Iter [600/600] Loss: 0.0953\n",
      "Epoch [4/5], Iter [100/600] Loss: 0.0089\n",
      "Epoch [4/5], Iter [200/600] Loss: 0.0121\n",
      "Epoch [4/5], Iter [300/600] Loss: 0.0560\n",
      "Epoch [4/5], Iter [400/600] Loss: 0.0060\n",
      "Epoch [4/5], Iter [500/600] Loss: 0.0140\n",
      "Epoch [4/5], Iter [600/600] Loss: 0.0047\n",
      "Epoch [5/5], Iter [100/600] Loss: 0.0033\n",
      "Epoch [5/5], Iter [200/600] Loss: 0.0141\n",
      "Epoch [5/5], Iter [300/600] Loss: 0.0087\n",
      "Epoch [5/5], Iter [400/600] Loss: 0.0155\n",
      "Epoch [5/5], Iter [500/600] Loss: 0.0049\n",
      "Epoch [5/5], Iter [600/600] Loss: 0.0286\n",
      "Test Accuracy of the model on the 10000 test images: 98 %\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Hyper Parameters\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST Dataset\n",
    "train_dataset = dsets.MNIST(root='./data/',\n",
    "                            train=True, \n",
    "                            transform=transforms.ToTensor(),\n",
    "                            download=True)\n",
    "\n",
    "test_dataset = dsets.MNIST(root='./data/',\n",
    "                           train=False, \n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# CNN Model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # [Conv > BN > ReLU > Pool] のブロックをまとめられる\n",
    "        # in:[N, 1, 28, 28] out:[N, 16, 14, 14]\n",
    "        self.layer1 = nn.Sequential(\n",
    "            # PyTorchではin_channel, out_channelの両方とも明示する必要あり\n",
    "            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n",
    "            # PyTorchではBatchNormも入力のfeatures数を明示する必要あり\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        # in:[N, 16, 14, 14] out:[N, 32, 7, 7]\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2))\n",
    "        # in:[N, 7*7*32] out:[N, 10]\n",
    "        self.fc = nn.Linear(7 * 7 * 32, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "#         print('in:', x.size())\n",
    "        out = self.layer1(x)\n",
    "#         print('out layer1:', out.size())\n",
    "        out = self.layer2(out)\n",
    "#         print('out layer2:', out.size())\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "#         print('out fc:', out.size())\n",
    "        return out\n",
    "\n",
    "cnn = CNN()\n",
    "\n",
    "# モデルの各層のテンソルサイズを見たいときはダミーの入力を渡すと簡単\n",
    "# images, labels = iter(test_loader).next()\n",
    "# cnn(Variable(images))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the Model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [%d/%d], Iter [%d/%d] Loss: %.4f' \n",
    "                   %(epoch + 1, num_epochs, i + 1, len(train_dataset) // batch_size, loss.data[0]))\n",
    "\n",
    "# Test the Model\n",
    "# BNを使っているときはモデルをevalモードにする\n",
    "cnn.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    images = Variable(images)\n",
    "    outputs = cnn(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted == labels).sum()\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "# Save the Trained Model\n",
    "torch.save(cnn.state_dict(), 'cnn.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
