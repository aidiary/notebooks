{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN VC\n",
    "\n",
    "- https://github.com/pritishyuvraj/Voice-Conversion-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download VCC2016 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-10-08 10:13:55--  https://datashare.is.ed.ac.uk/bitstream/handle/10283/2211/vcc2016_training.zip\n",
      "Resolving datashare.is.ed.ac.uk (datashare.is.ed.ac.uk)... 129.215.41.53\n",
      "Connecting to datashare.is.ed.ac.uk (datashare.is.ed.ac.uk)|129.215.41.53|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 149702219 (143M) [application/zip]\n",
      "Saving to: ‘vcc2016_training.zip’\n",
      "\n",
      "vcc2016_training.zi 100%[===================>] 142.77M   175KB/s    in 14m 8s  \n",
      "\n",
      "2020-10-08 10:28:04 (172 KB/s) - ‘vcc2016_training.zip’ saved [149702219/149702219]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://datashare.is.ed.ac.uk/bitstream/handle/10283/2211/vcc2016_training.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-10-08 10:28:04--  https://datashare.is.ed.ac.uk/bitstream/handle/10283/2211/evaluation_all.zip\n",
      "Resolving datashare.is.ed.ac.uk (datashare.is.ed.ac.uk)... 129.215.41.53\n",
      "Connecting to datashare.is.ed.ac.uk (datashare.is.ed.ac.uk)|129.215.41.53|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 45534075 (43M) [application/zip]\n",
      "Saving to: ‘evaluation_all.zip’\n",
      "\n",
      "evaluation_all.zip  100%[===================>]  43.42M   179KB/s    in 4m 17s  \n",
      "\n",
      "2020-10-08 10:32:23 (173 KB/s) - ‘evaluation_all.zip’ saved [45534075/45534075]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://datashare.is.ed.ac.uk/bitstream/handle/10283/2211/evaluation_all.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JVS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -s ./data/jvs_ver1/jvs002 data/src\n",
    "!ln -s ./data/jvs_ver1/jvs010 data/tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20170104210653.jpg  20170104210705.jpg\t       jvs_ver1  src   tgt\n",
      "20170104210658.jpg  imagenet_class_index.json  mnist\t svhn\n"
     ]
    }
   ],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理\n",
    "\n",
    "- とりあえずparallelデータのままで訓練してみる\n",
    "- 実際は、前半の50、後半の50でデータをわける"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_A_dir = './data/src'  # JVS002\n",
    "train_B_dir = './data/tgt'  # JVS010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "\n",
    "def load_wavs(wav_dir, sr):\n",
    "    wavs = list()\n",
    "    for file in os.listdir(wav_dir):\n",
    "        file_path = os.path.join(wav_dir, file)\n",
    "        wav, _ = librosa.load(file_path, sr=sr, mono=True)\n",
    "        wavs.append(wav)\n",
    "    return wavs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavs_A = load_wavs(train_A_dir, sr=16000)\n",
    "wavs_B = load_wavs(train_B_dir, sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyworld\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/91/1b3ebd3840a76e50b3695a9d8515a44303a90c74ae13e474647d984d1e12/pyworld-0.2.11.post0.tar.gz (222kB)\n",
      "\u001b[K     |████████████████████████████████| 225kB 6.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /home/koichiro_mori/miniconda3/lib/python3.7/site-packages (from pyworld) (1.18.1)\n",
      "Requirement already satisfied: cython>=0.24.0 in /home/koichiro_mori/miniconda3/lib/python3.7/site-packages (from pyworld) (0.29.17)\n",
      "Building wheels for collected packages: pyworld\n",
      "  Building wheel for pyworld (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyworld: filename=pyworld-0.2.11.post0-cp37-cp37m-linux_x86_64.whl size=667594 sha256=ccbffcc480981feabedcf916d4f95b99024f5a930ed384cdc55af97d88a778fd\n",
      "  Stored in directory: /home/koichiro_mori/.cache/pip/wheels/dd/af/e5/28059a621233a9204e9322986b2afddb90976ad5b1c05d76d0\n",
      "Successfully built pyworld\n",
      "Installing collected packages: pyworld\n",
      "Successfully installed pyworld-0.2.11.post0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyworld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pysptk in /home/koichiro_mori/miniconda3/lib/python3.7/site-packages (0.1.18)\n",
      "Requirement already satisfied: decorator in /home/koichiro_mori/miniconda3/lib/python3.7/site-packages (from pysptk) (4.4.1)\n",
      "Requirement already satisfied: scipy in /home/koichiro_mori/miniconda3/lib/python3.7/site-packages (from pysptk) (1.4.1)\n",
      "Requirement already satisfied: six in /home/koichiro_mori/miniconda3/lib/python3.7/site-packages (from pysptk) (1.12.0)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/koichiro_mori/miniconda3/lib/python3.7/site-packages (from scipy->pysptk) (1.18.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pysptk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyworld\n",
    "import pysptk\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def world_encode_data(wave, fs, frame_period=5.0, num_mcep=24):\n",
    "    f0s = list()\n",
    "    timeaxes = list()\n",
    "    sps = list()\n",
    "    aps = list()\n",
    "    coded_sps = list()\n",
    "    for wav in tqdm(wave):\n",
    "        wav = wav.astype(np.float64)\n",
    "\n",
    "        f0, timeaxis = pyworld.harvest(wav, fs, frame_period=frame_period, f0_floor=71.0, f0_ceil=800.0)\n",
    "        sp = pyworld.cheaptrick(wav, f0, timeaxis, fs)\n",
    "        ap = pyworld.d4c(wav, f0, timeaxis, fs)\n",
    "\n",
    "        # 24次元が抽出される\n",
    "        # メルフィルタを使ってないためmcepではなさそう\n",
    "        coded_sp = pyworld.code_spectral_envelope(sp, fs, num_mcep)\n",
    "\n",
    "#         alpha = pysptk.util.mcepalpha(fs)\n",
    "#         mcep = pysptk.sp2mc(sp, mcep_order, alpha)\n",
    "\n",
    "        f0s.append(f0)\n",
    "        timeaxes.append(timeaxis)\n",
    "        sps.append(sp)\n",
    "        aps.append(ap)\n",
    "        coded_sps.append(coded_sp)\n",
    "\n",
    "    return f0s, timeaxes, sps, aps, coded_sps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [difference between pysptk.sp2mc AND pyworld.code_spectral_envelope #74](https://github.com/r9y9/pysptk/issues/74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_mcep = 24\n",
    "sampling_rate = 16000\n",
    "frame_period = 5.0\n",
    "n_frames = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lf0, mcep, apの抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [03:32<00:00,  2.12s/it]\n",
      "100%|██████████| 100/100 [03:48<00:00,  2.29s/it]\n"
     ]
    }
   ],
   "source": [
    "f0s_A, timeaxes_A, sps_A, aps_A, coded_sps_A = world_encode_data(wavs_A,\n",
    "                                                                 fs=sampling_rate,\n",
    "                                                                 frame_period=frame_period,\n",
    "                                                                 num_mcep=num_mcep)\n",
    "\n",
    "f0s_B, timeaxes_B, sps_B, aps_B, coded_sps_B = world_encode_data(wavs_B,\n",
    "                                                                 fs=sampling_rate,\n",
    "                                                                 frame_period=frame_period,\n",
    "                                                                 num_mcep=num_mcep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((820,), (820, 513), (820, 513), (820, 24))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f0s_A[0].shape, sps_A[0].shape, aps_A[0].shape, coded_sps_A[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lf0の統計量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logf0_statistics(f0s):\n",
    "    # Note: np.ma.log() calculating log on masked array (for incomplete or invalid entries in array)\n",
    "    log_f0s_concatenated = np.ma.log(np.concatenate(f0s))\n",
    "    log_f0s_mean = log_f0s_concatenated.mean()\n",
    "    log_f0s_std = log_f0s_concatenated.std()\n",
    "    return log_f0s_mean, log_f0s_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.360063345512454 0.29044769128393916\n",
      "5.623051827437946 0.3459336982720282\n"
     ]
    }
   ],
   "source": [
    "log_f0s_mean_A, log_f0s_std_A = logf0_statistics(f0s_A)\n",
    "log_f0s_mean_B, log_f0s_std_B = logf0_statistics(f0s_B)\n",
    "print(log_f0s_mean_A, log_f0s_std_A)\n",
    "print(log_f0s_mean_B, log_f0s_std_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mcepの統計量の算出と標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coded_sps_normalization_fit_transform(coded_sps):\n",
    "    coded_sps_concatenated = np.concatenate(coded_sps, axis=1)\n",
    "    coded_sps_mean = np.mean(coded_sps_concatenated, axis=1, keepdims=True)\n",
    "    coded_sps_std = np.std(coded_sps_concatenated, axis=1, keepdims=True)\n",
    "    coded_sps_normalized = list()\n",
    "    for coded_sp in coded_sps:\n",
    "        coded_sps_normalized.append(\n",
    "            (coded_sp - coded_sps_mean) / coded_sps_std)\n",
    "    return coded_sps_normalized, coded_sps_mean, coded_sps_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "coded_sps_A_norm, coded_sps_A_mean, coded_sps_A_std = coded_sps_normalization_fit_transform(\n",
    "    [x.T for x in coded_sps_A])\n",
    "\n",
    "coded_sps_B_norm, coded_sps_B_mean, coded_sps_B_std = coded_sps_normalization_fit_transform(\n",
    "    [x.T for x in coded_sps_B])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24, 820), (24, 820))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coded_sps_A_norm[0].shape, coded_sps_A_norm[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('logf0s_normalization.npz',\n",
    "         mean_A=log_f0s_mean_A,\n",
    "         std_A=log_f0s_std_A,\n",
    "         mean_B=log_f0s_mean_B,\n",
    "         std_B=log_f0s_std_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('mcep_normalization.npz',\n",
    "         mean_A=coded_sps_A_mean,\n",
    "         std_A=coded_sps_A_std,\n",
    "         mean_B=coded_sps_B_mean,\n",
    "         std_B=coded_sps_B_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('coded_sps_A_norm.pickle', 'wb') as f:\n",
    "    pickle.dump(coded_sps_A_norm, f)\n",
    "\n",
    "with open('coded_sps_B_norm.pickle', 'wb') as f:\n",
    "    pickle.dump(coded_sps_B_norm, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, datasetA, datasetB, n_frames=128):\n",
    "        # n_framesは切り出すフレーム長\n",
    "        self.datasetA = datasetA\n",
    "        self.datasetB = datasetB\n",
    "        self.n_frames = n_frames\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # データサンプル数の小さい方\n",
    "        num_samples = min(len(datasetA), len(datasetB))\n",
    "        n_frames = self.n_frames\n",
    "\n",
    "        # この処理は無駄が多そう\n",
    "        # 1.srcからランダムに選択\n",
    "        # 2.tgtからランダムに選択\n",
    "        # 3.n_framesでランダムに切り出して返す\n",
    "        # だけでOKかも\n",
    "\n",
    "        # 毎回、src/tgtに使う音声ペアの組み合わせをシャッフルする\n",
    "        train_dataA_idx = np.arange(len(datasetA))\n",
    "        train_dataB_idx = np.arange(len(datasetB))\n",
    "        np.random.shuffle(train_dataA_idx)\n",
    "        np.random.shuffle(train_dataB_idx)\n",
    "        train_dataA_idx = train_dataA_idx[:num_samples]\n",
    "        train_dataB_idx = train_dataB_idx[:num_samples]\n",
    "        \n",
    "        train_dataA = list()\n",
    "        train_dataB = list()\n",
    "\n",
    "        # srcとtgtでランダムに音声ファイルのペアを作る\n",
    "        # パラレルデータになってない（系列長も異なる）\n",
    "        for idx_A, idx_B in zip(train_dataA_idx, train_dataB_idx):\n",
    "            dataA = datasetA[idx_A]\n",
    "            frames_A_total = dataA.shape[1]\n",
    "            # 音声のフレーム長が切り出すフレーム長より長い必要がある\n",
    "            assert frames_A_total >= n_frames\n",
    "            # ランダムにフレームを切り出す\n",
    "            startA = np.random.randint(frames_A_total - n_frames + 1)\n",
    "            endA = startA + n_frames\n",
    "            train_dataA.append(dataA[:, startA:endA])\n",
    "\n",
    "            dataB = datasetB[idx_B]\n",
    "            frames_B_total = dataB.shape[1]\n",
    "            # 音声のフレーム長が切り出すフレーム長より長い必要がある\n",
    "            assert frames_B_total >= n_frames\n",
    "            # ランダムにフレームを切り出す\n",
    "            startB = np.random.randint(frames_B_total - n_frames + 1)\n",
    "            endB = startB + n_frames\n",
    "            train_dataB.append(dataB[:, startB:endB])\n",
    "        \n",
    "        train_dataA = np.array(train_dataA)\n",
    "        train_dataB = np.array(train_dataB)\n",
    "        \n",
    "        # 結局、indexのしか使わないので↑の処理は無意味…\n",
    "        return train_dataA[index], train_dataB[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.datasetA), len(self.datasetB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "trainA = np.random.randn(162, 24, 554)  # (size, mcep_dim, frame_size)\n",
    "trainB = np.random.randn(158, 24, 554)\n",
    "dataset = TrainingDataset(trainA, trainB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100\n",
      "(24, 820) (24, 1615)\n",
      "(24, 823) (24, 1722)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('coded_sps_A_norm.pickle', 'rb') as f:\n",
    "    datasetA = pickle.load(f)\n",
    "with open('coded_sps_B_norm.pickle', 'rb') as f:\n",
    "    datasetB = pickle.load(f)\n",
    "print(len(datasetA), len(datasetB))\n",
    "print(datasetA[0].shape, datasetA[1].shape)\n",
    "print(datasetB[0].shape, datasetB[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24, 128), (24, 128))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = TrainingDataset(datasetA, datasetB, n_frames=128)\n",
    "dataset[0][0].shape, dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 24, 128])\n",
      "torch.Size([2, 24, 128])\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "batch = iter(train_loader).next()\n",
    "print(batch[0].shape)  # src\n",
    "print(batch[1].shape)  # tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CycleGAN-VC Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GLU, self).__init__()\n",
    "        # PyTorchのGLUと何が違う？\n",
    "    \n",
    "    def forward(self, input):\n",
    "        return input * torch.sigmoid(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelShuffle(nn.Module):\n",
    "    def __init__(self, upscale_factor):\n",
    "        super(PixelShuffle, self).__init__()\n",
    "        # PyTorchのPixelShuffleは4DTensor入力なので3DTensor入力の自作\n",
    "        self.upscale_factor = upscale_factor\n",
    "    \n",
    "    def forward(self, input):\n",
    "        n = input.shape[0]\n",
    "        c_out = input.shape[1] // 2\n",
    "        w_new = input.shape[2] * 2\n",
    "        return input.view(n, c_out, w_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        \n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Conv1d(in_channels,\n",
    "                      out_channels,\n",
    "                      kernel_size=kernel_size,\n",
    "                      stride=1,\n",
    "                      padding=padding),\n",
    "            nn.InstanceNorm1d(num_features=out_channels, affine=True),\n",
    "            GLU(),\n",
    "            nn.Conv1d(in_channels=out_channels,\n",
    "                     out_channels=in_channels,\n",
    "                     kernel_size=kernel_size,\n",
    "                     stride=1,\n",
    "                     padding=padding),\n",
    "            nn.InstanceNorm1d(num_features=in_channels, affine=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return input + self.layers(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # 入力はmcepの24次元 (batch, mcep_dim, n_frames)\n",
    "        self.conv1 = nn.Sequential(nn.Conv1d(in_channels=24,\n",
    "                                             out_channels=128,\n",
    "                                             kernel_size=15,\n",
    "                                             stride=1,\n",
    "                                             padding=7),\n",
    "                                   GLU())\n",
    "        \n",
    "        # Downsample Layer\n",
    "        # 系列長を短くしていく\n",
    "        self.downsample1 = self.downsample(in_channels=128,\n",
    "                                           out_channels=256,\n",
    "                                           kernel_size=5,\n",
    "                                           stride=2,\n",
    "                                           padding=1)\n",
    "        \n",
    "        self.downsample2 = self.downsample(in_channels=256,\n",
    "                                           out_channels=512,\n",
    "                                           kernel_size=5,\n",
    "                                           stride=2,\n",
    "                                           padding=2)\n",
    "        \n",
    "        # Residual Blocks\n",
    "        # ResidualLayerは出力はin_channelsになるので注意\n",
    "        residual_layers = []\n",
    "        for i in range(6):\n",
    "            residual_layers.append(ResidualLayer(in_channels=512,\n",
    "                                                 out_channels=1024,\n",
    "                                                 kernel_size=3,\n",
    "                                                 stride=1,\n",
    "                                                 padding=1))\n",
    "        self.residual_layers = nn.ModuleList(residual_layers)\n",
    "        \n",
    "        # Upsample Layer\n",
    "        self.upsample1 = self.upsample(in_channels=512,\n",
    "                                       out_channels=1024,\n",
    "                                       kernel_size=5,\n",
    "                                       stride=1,\n",
    "                                       padding=2)\n",
    "\n",
    "        self.upsample2 = self.upsample(in_channels=512,\n",
    "                                       out_channels=512,\n",
    "                                       kernel_size=5,\n",
    "                                       stride=1,\n",
    "                                       padding=2)\n",
    "        \n",
    "        self.last_conv_layer = nn.Conv1d(in_channels=256,\n",
    "                                         out_channels=24,\n",
    "                                         kernel_size=15,\n",
    "                                         stride=1,\n",
    "                                         padding=7)\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.conv1(input)\n",
    "        output = self.downsample1(output)\n",
    "        output = self.downsample2(output)\n",
    "        for i in range(6):\n",
    "            output = self.residual_layers[i](output)\n",
    "        output = self.upsample1(output)\n",
    "        output = self.upsample2(output)\n",
    "        output = self.last_conv_layer(output)\n",
    "        return output\n",
    "\n",
    "    def downsample(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        # Conv => InstanceNorm => GLU のブロック\n",
    "        conv_layer = nn.Sequential(nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "                                   nn.InstanceNorm1d(num_features=out_channels, affine=True),\n",
    "                                   GLU())\n",
    "        return conv_layer\n",
    "\n",
    "    def upsample(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        conv_layer = nn.Sequential(nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "                                   PixelShuffle(upscale_factor=2),\n",
    "                                   nn.InstanceNorm1d(num_features=out_channels // 2, affine=True),\n",
    "                                   GLU())\n",
    "        return conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1             [-1, 128, 128]          46,208\n",
      "               GLU-2             [-1, 128, 128]               0\n",
      "            Conv1d-3              [-1, 256, 63]         164,096\n",
      "    InstanceNorm1d-4              [-1, 256, 63]             512\n",
      "               GLU-5              [-1, 256, 63]               0\n",
      "            Conv1d-6              [-1, 512, 32]         655,872\n",
      "    InstanceNorm1d-7              [-1, 512, 32]           1,024\n",
      "               GLU-8              [-1, 512, 32]               0\n",
      "            Conv1d-9             [-1, 1024, 32]       1,573,888\n",
      "   InstanceNorm1d-10             [-1, 1024, 32]           2,048\n",
      "              GLU-11             [-1, 1024, 32]               0\n",
      "           Conv1d-12              [-1, 512, 32]       1,573,376\n",
      "   InstanceNorm1d-13              [-1, 512, 32]           1,024\n",
      "    ResidualLayer-14              [-1, 512, 32]               0\n",
      "           Conv1d-15             [-1, 1024, 32]       1,573,888\n",
      "   InstanceNorm1d-16             [-1, 1024, 32]           2,048\n",
      "              GLU-17             [-1, 1024, 32]               0\n",
      "           Conv1d-18              [-1, 512, 32]       1,573,376\n",
      "   InstanceNorm1d-19              [-1, 512, 32]           1,024\n",
      "    ResidualLayer-20              [-1, 512, 32]               0\n",
      "           Conv1d-21             [-1, 1024, 32]       1,573,888\n",
      "   InstanceNorm1d-22             [-1, 1024, 32]           2,048\n",
      "              GLU-23             [-1, 1024, 32]               0\n",
      "           Conv1d-24              [-1, 512, 32]       1,573,376\n",
      "   InstanceNorm1d-25              [-1, 512, 32]           1,024\n",
      "    ResidualLayer-26              [-1, 512, 32]               0\n",
      "           Conv1d-27             [-1, 1024, 32]       1,573,888\n",
      "   InstanceNorm1d-28             [-1, 1024, 32]           2,048\n",
      "              GLU-29             [-1, 1024, 32]               0\n",
      "           Conv1d-30              [-1, 512, 32]       1,573,376\n",
      "   InstanceNorm1d-31              [-1, 512, 32]           1,024\n",
      "    ResidualLayer-32              [-1, 512, 32]               0\n",
      "           Conv1d-33             [-1, 1024, 32]       1,573,888\n",
      "   InstanceNorm1d-34             [-1, 1024, 32]           2,048\n",
      "              GLU-35             [-1, 1024, 32]               0\n",
      "           Conv1d-36              [-1, 512, 32]       1,573,376\n",
      "   InstanceNorm1d-37              [-1, 512, 32]           1,024\n",
      "    ResidualLayer-38              [-1, 512, 32]               0\n",
      "           Conv1d-39             [-1, 1024, 32]       1,573,888\n",
      "   InstanceNorm1d-40             [-1, 1024, 32]           2,048\n",
      "              GLU-41             [-1, 1024, 32]               0\n",
      "           Conv1d-42              [-1, 512, 32]       1,573,376\n",
      "   InstanceNorm1d-43              [-1, 512, 32]           1,024\n",
      "    ResidualLayer-44              [-1, 512, 32]               0\n",
      "           Conv1d-45             [-1, 1024, 32]       2,622,464\n",
      "     PixelShuffle-46              [-1, 512, 64]               0\n",
      "   InstanceNorm1d-47              [-1, 512, 64]           1,024\n",
      "              GLU-48              [-1, 512, 64]               0\n",
      "           Conv1d-49              [-1, 512, 64]       1,311,232\n",
      "     PixelShuffle-50             [-1, 256, 128]               0\n",
      "   InstanceNorm1d-51             [-1, 256, 128]             512\n",
      "              GLU-52             [-1, 256, 128]               0\n",
      "           Conv1d-53              [-1, 24, 128]          92,184\n",
      "================================================================\n",
      "Total params: 23,797,144\n",
      "Trainable params: 23,797,144\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 9.77\n",
      "Params size (MB): 90.78\n",
      "Estimated Total Size (MB): 100.56\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "g = Generator().to(device)\n",
    "summary(g, (24, 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 24, 128])\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "g = Generator().to(device)\n",
    "input = torch.rand((2, 24, 128)).to(device)\n",
    "output = g(input)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.conv_layer1 = nn.Sequential(nn.Conv2d(in_channels=1,\n",
    "                                                   out_channels=128,\n",
    "                                                   kernel_size=[3, 3],\n",
    "                                                   stride=[1, 2],\n",
    "                                                   padding=[1, 1]),\n",
    "                                         GLU())\n",
    "        \n",
    "        self.downsample1 = self.downsample(in_channels=128,\n",
    "                                           out_channels=256,\n",
    "                                           kernel_size=[3, 3],\n",
    "                                           stride=[2, 2],\n",
    "                                           padding=[1, 1])\n",
    "        self.downsample2 = self.downsample(in_channels=256,\n",
    "                                           out_channels=512,\n",
    "                                           kernel_size=[3, 3],\n",
    "                                           stride=[2, 2],\n",
    "                                           padding=[1, 1])\n",
    "        self.downsample3 = self.downsample(in_channels=512,\n",
    "                                           out_channels=1024,\n",
    "                                           kernel_size=[6, 3],\n",
    "                                           stride=[1, 2],\n",
    "                                           padding=[3, 1])\n",
    "        self.fc = nn.Linear(1024, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # discriminatorは4DTensor入力にするためchannelsを追加\n",
    "        # [batch_size, num_features, num_frames] => [batch_size, 1, num_features, num_frames]\n",
    "        input = input.unsqueeze(1)\n",
    "        output = self.conv_layer1(input)\n",
    "        output = self.downsample1(output)\n",
    "        output = self.downsample2(output)\n",
    "        output = self.downsample3(output)\n",
    "        output = output.permute(0, 2, 3, 1).contiguous()\n",
    "        output = torch.sigmoid(self.fc(output))\n",
    "        return output\n",
    "    \n",
    "    def downsample(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        conv_layer = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "                                   nn.InstanceNorm2d(num_features=out_channels, affine=True),\n",
    "                                   GLU())\n",
    "        return conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 24, 64]           1,280\n",
      "               GLU-2          [-1, 128, 24, 64]               0\n",
      "            Conv2d-3          [-1, 256, 12, 32]         295,168\n",
      "    InstanceNorm2d-4          [-1, 256, 12, 32]             512\n",
      "               GLU-5          [-1, 256, 12, 32]               0\n",
      "            Conv2d-6           [-1, 512, 6, 16]       1,180,160\n",
      "    InstanceNorm2d-7           [-1, 512, 6, 16]           1,024\n",
      "               GLU-8           [-1, 512, 6, 16]               0\n",
      "            Conv2d-9           [-1, 1024, 7, 8]       9,438,208\n",
      "   InstanceNorm2d-10           [-1, 1024, 7, 8]           2,048\n",
      "              GLU-11           [-1, 1024, 7, 8]               0\n",
      "           Linear-12              [-1, 7, 8, 1]           1,025\n",
      "================================================================\n",
      "Total params: 10,919,425\n",
      "Trainable params: 10,919,425\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 7.69\n",
      "Params size (MB): 41.65\n",
      "Estimated Total Size (MB): 49.35\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "g = Discriminator().to(device)\n",
    "summary(g, (24, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs/Oct09_13-26-10_dlgdev0001\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()\n",
    "print(writer.log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5000\n",
    "batch_size = 1\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "import pickle\n",
    "import torch.utils.data\n",
    "\n",
    "with open('coded_sps_A_norm.pickle', 'rb') as f:\n",
    "    datasetA = pickle.load(f)\n",
    "\n",
    "with open('coded_sps_B_norm.pickle', 'rb') as f:\n",
    "    datasetB = pickle.load(f)\n",
    "\n",
    "train_data = TrainingDataset(datasetA, datasetB, n_frames=128)\n",
    "train_loader = torch.utils.data.DataLoader(train_data,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=8,\n",
    "                                           drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.360063345512454\n",
      "0.29044769128393916\n",
      "5.623051827437946\n",
      "0.3459336982720282\n",
      "(24, 1)\n",
      "(24, 1)\n",
      "(24, 1)\n",
      "(24, 1)\n"
     ]
    }
   ],
   "source": [
    "# scaler\n",
    "logf0s_normalization = np.load('logf0s_normalization.npz')\n",
    "print(logf0s_normalization['mean_A'])\n",
    "print(logf0s_normalization['std_A'])\n",
    "print(logf0s_normalization['mean_B'])\n",
    "print(logf0s_normalization['std_B'])\n",
    "\n",
    "mcep_normalization = np.load('mcep_normalization.npz')\n",
    "print(mcep_normalization['mean_A'].shape)\n",
    "print(mcep_normalization['std_A'].shape)\n",
    "print(mcep_normalization['mean_B'].shape)\n",
    "print(mcep_normalization['std_B'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "generator_A2B = Generator().to(device)\n",
    "generator_B2A = Generator().to(device)\n",
    "discriminator_A = Discriminator().to(device)\n",
    "discriminator_B = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial learning rates\n",
    "# 論文の設定と同じ\n",
    "g_lr = 0.0002\n",
    "d_lr = 0.0001\n",
    "\n",
    "# learning rate decay\n",
    "# 最初の200000itersは変えない、次の200000itersは線形に落としていく\n",
    "g_lr_decay = g_lr / 200000\n",
    "d_lr_decay = d_lr / 200000\n",
    "start_decay = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# optimizer\n",
    "g_params = list(generator_A2B.parameters()) + list(generator_B2A.parameters())\n",
    "d_params = list(discriminator_A.parameters()) + list(discriminator_B.parameters())\n",
    "\n",
    "g_optim = optim.Adam(g_params, lr=g_lr, betas=(0.5, 0.999))\n",
    "d_optim = optim.Adam(d_params, lr=d_lr, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss weight\n",
    "cycle_loss_lambda = 10\n",
    "identity_loss_lambda = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_lr_rate(optimizer, lr, lr_decay):\n",
    "    new_lr = max(0.0, lr - lr_decay)\n",
    "    for param_groups in optimizer.param_groups:\n",
    "        param_groups['lr'] = new_lr\n",
    "    return new_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100 g_loss: 27.35570526123047 d_loss: 0.0625004917383194\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-301-dc001646a8b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mg_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0md_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mg_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mg_optim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# train\n",
    "global_iters = 0\n",
    "\n",
    "for epoch in range(1, num_epochs):\n",
    "    for i, (realA, realB) in enumerate(train_loader):\n",
    "        global_iters += 1\n",
    "\n",
    "        if global_iters > 10000:\n",
    "            identity_loss_lambda = 0\n",
    "\n",
    "        if global_iters > start_decay:\n",
    "            g_lr = adjust_lr_rate(g_optim, g_lr, g_lr_decay)\n",
    "            d_lr = adjust_lr_rate(d_optim, d_lr, d_lr_decay)\n",
    "\n",
    "        realA, realB = realA.float().to(device), realB.float().to(device)\n",
    "        \n",
    "        # train generator\n",
    "        fakeB = generator_A2B(realA)\n",
    "        cycleA = generator_B2A(fakeB)\n",
    "        \n",
    "        fakeA = generator_B2A(realB)\n",
    "        cycleB = generator_A2B(fakeA)\n",
    "        \n",
    "        identityA = generator_B2A(realA)\n",
    "        identityB = generator_A2B(realB)\n",
    "        \n",
    "        d_fakeA = discriminator_A(fakeA)  # [-1, 7, 8, 1]\n",
    "        d_fakeB = discriminator_B(fakeB)  # [-1, 7, 8, 1]\n",
    "        \n",
    "        # cycle consistency loss\n",
    "        cycle_loss = torch.mean(torch.abs(realA - cycleA)) + torch.mean(torch.abs(realB - cycleB))\n",
    "        \n",
    "        # identity loss\n",
    "        identity_loss = torch.mean(torch.abs(realA - identityA)) + torch.mean(torch.abs(realB - identityB))\n",
    "        \n",
    "        # adversarial loss (fake入力の識別結果が1に近づいてほしい）)\n",
    "        adv_loss = torch.mean((1 - d_fakeA) ** 2) + torch.mean((1 - d_fakeB) ** 2)\n",
    "        \n",
    "        # total generator loss\n",
    "        g_loss = adv_loss + cycle_loss_lambda * cycle_loss + identity_loss_lambda * identity_loss\n",
    "\n",
    "        g_optim.zero_grad()\n",
    "        d_optim.zero_grad()\n",
    "        g_loss.backward(retain_graph=True)\n",
    "        g_optim.step()\n",
    "\n",
    "        writer.add_scalar('g_loss', g_loss.item(), global_iters)\n",
    "        \n",
    "        # train discriminator\n",
    "        d_realA = discriminator_A(realA)\n",
    "        d_realB = discriminator_B(realB)\n",
    "        \n",
    "        fakeA = generator_B2A(realB)\n",
    "        d_fake_A = discriminator_A(fakeA)\n",
    "        \n",
    "        fakeB = generator_A2B(realA)\n",
    "        d_fake_B = discriminator_B(fakeB)\n",
    "        \n",
    "        d_loss_realA = torch.mean((1 - d_realA) ** 2)        \n",
    "        d_loss_fakeA = torch.mean((0 - d_fakeA) ** 2)\n",
    "        d_loss_A = d_loss_realA + d_loss_fakeA\n",
    "\n",
    "        d_loss_realB = torch.mean((1 - d_realB) ** 2)        \n",
    "        d_loss_fakeB = torch.mean((0 - d_fakeB) ** 2)\n",
    "        d_loss_B = d_loss_realB + d_loss_fakeB\n",
    "        \n",
    "        d_loss = d_loss_A + d_loss_B\n",
    "        \n",
    "        writer.add_scalar('d_loss', d_loss.item(), global_iters)\n",
    "        \n",
    "        g_optim.zero_grad()\n",
    "        d_optim.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optim.step()\n",
    "    \n",
    "    if epoch % 100 == 0 and epoch != 0:\n",
    "        print('Epoch {} g_loss: {} d_loss: {}'.format(epoch, g_loss.item(), d_loss.item()))\n",
    "        checkpoint_path = os.path.join(writer.log_dir, 'checkpoint_epoch{:03d}.pth'.format(epoch))\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'generator_A2B_state_dict': generator_A2B.state_dict(),\n",
    "            'generator_B2A_state_dict': generator_B2A.state_dict(),\n",
    "            'discriminator_A_state_dict': discriminator_A.state_dict(),\n",
    "            'discriminator_B_state_dict': discriminator_B.state_dict(),\n",
    "            'g_optim': g_optim.state_dict(),\n",
    "            'd_optim': d_optim.state_dict()\n",
    "        }, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
