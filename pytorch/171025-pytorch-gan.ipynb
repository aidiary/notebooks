{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 引数で指定したモデルの各層に対して特定の初期化を行う\n",
    "def initialize_weights(net):\n",
    "    for i, m in enumerate(net.modules()):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.ConvTranspose2d):\n",
    "            # パラメータのTensorをランダムに初期化\n",
    "            # バイアスは0に初期化\n",
    "            # _がついているのでinplaceで置き換え\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.weight.data.normal_(0, 0.02)\n",
    "            m.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator (\n",
      "  (fc): Sequential (\n",
      "    (0): Linear (62 -> 1024)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): ReLU ()\n",
      "    (3): Linear (1024 -> 6272)\n",
      "    (4): BatchNorm1d(6272, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (5): ReLU ()\n",
      "  )\n",
      "  (deconv): Sequential (\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): ReLU ()\n",
      "    (3): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): Sigmoid ()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, dataset='mnist'):\n",
    "        super(generator, self).__init__()\n",
    "        if dataset == 'mnist':\n",
    "            self.input_height = 28\n",
    "            self.input_width = 28\n",
    "            self.input_dim = 62  # zの乱数の次元\n",
    "            self.output_dim = 1\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            # 入力の1/4のサイズに縮小\n",
    "            # deconvで4倍に拡大するため\n",
    "            nn.Linear(1024, 128 * (self.input_height // 4) * (self.input_width // 4)),\n",
    "            nn.BatchNorm1d(128 * (self.input_height // 4) * (self.input_width // 4)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, self.output_dim, 4, 2, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        # 層によってパラメータの初期化方法を変える\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.fc(input)\n",
    "        x = x.view(-1, 128, (self.input_height // 4), (self.input_width // 4))\n",
    "        x = self.deconv(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "G = generator('mnist')\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__gt__', '__hash__', '__init__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_all_buffers', '_apply', '_backend', '_backward_hooks', '_buffers', '_forward_hooks', '_forward_pre_hooks', '_modules', '_parameters', 'add_module', 'apply', 'children', 'conv', 'cpu', 'cuda', 'double', 'dump_patches', 'eval', 'fc', 'float', 'forward', 'half', 'input_dim', 'input_height', 'input_width', 'load_state_dict', 'modules', 'named_children', 'named_modules', 'named_parameters', 'output_dim', 'parameters', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_parameter', 'share_memory', 'state_dict', 'train', 'training', 'type', 'zero_grad']\n",
      "discriminator (\n",
      "  (conv): Sequential (\n",
      "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU (0.2)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (4): LeakyReLU (0.2)\n",
      "  )\n",
      "  (fc): Sequential (\n",
      "    (0): Linear (6272 -> 1024)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): LeakyReLU (0.2)\n",
      "    (3): Linear (1024 -> 1)\n",
      "    (4): Sigmoid ()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, dataset='mnist'):\n",
    "        super(discriminator, self).__init__()\n",
    "        if dataset == 'mnist':\n",
    "            self.input_height = 28\n",
    "            self.input_width = 28\n",
    "            self.input_dim = 1\n",
    "            self.output_dim = 1\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * (self.input_height // 4) * (self.input_width // 4), 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, self.output_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        initialize_weights(self)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = self.conv(input)\n",
    "        x = x.view(-1, 128 * (self.input_height // 4) * (self.input_width // 4))\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "D = discriminator('mnist')\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator (\n",
      "  (fc): Sequential (\n",
      "    (0): Linear (62 -> 1024)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): ReLU ()\n",
      "    (3): Linear (1024 -> 6272)\n",
      "    (4): BatchNorm1d(6272, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (5): ReLU ()\n",
      "  )\n",
      "  (deconv): Sequential (\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): ReLU ()\n",
      "    (3): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): Sigmoid ()\n",
      "  )\n",
      ")\n",
      "discriminator (\n",
      "  (conv): Sequential (\n",
      "    (0): Conv2d(1, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU (0.2)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (4): LeakyReLU (0.2)\n",
      "  )\n",
      "  (fc): Sequential (\n",
      "    (0): Linear (6272 -> 1024)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): LeakyReLU (0.2)\n",
      "    (3): Linear (1024 -> 1)\n",
      "    (4): Sigmoid ()\n",
      "  )\n",
      ")\n",
      "training start!!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-7e293bb5d79b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-115-7e293bb5d79b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_hist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'D_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0mD_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/koichiro.mori/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \"\"\"\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/koichiro.mori/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 98\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class GAN(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.epoch = 25\n",
    "        self.sample_num = 16\n",
    "        self.batch_size = 64\n",
    "        self.save_dir = 'models'\n",
    "        self.result_dir = 'results'\n",
    "        self.dataset = 'mnist'\n",
    "        self.log_dir = 'logs'\n",
    "        self.model_name = 'GAN'\n",
    "        self.gpu_mode = torch.cuda.is_available()\n",
    "\n",
    "        self.G = generator(self.dataset)\n",
    "        self.D = discriminator(self.dataset)\n",
    "\n",
    "        self.G_optimizer = optim.Adam(self.G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        self.D_optimizer = optim.Adam(self.D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        \n",
    "        if self.gpu_mode:\n",
    "            self.G.cuda()\n",
    "            self.D.cuda()\n",
    "            # Dは2値分類なので Binary Cross Entropy Loss を使う\n",
    "            # lossはcuda()する必要あるのか？\n",
    "            self.BCE_loss = nn.BCELoss().cuda()\n",
    "        else:\n",
    "            self.BCE_loss = nn.BCELoss()\n",
    "        \n",
    "        print(self.G)\n",
    "        print(self.D)\n",
    "\n",
    "        # load dataset\n",
    "        if self.dataset == 'mnist':\n",
    "            d = datasets.MNIST('data/mnist', train=True, download=True,\n",
    "                               transform=transforms.Compose([transforms.ToTensor()]))\n",
    "            self.data_loader = DataLoader(d, batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        self.z_dim = 62\n",
    "        \n",
    "        if self.gpu_mode:\n",
    "            self.sample_z = Variable(torch.rand((self.batch_size, self.z_dim)).cuda(), volatile=True)\n",
    "        else:\n",
    "            self.sample_z = Variable(torch.rand((self.batch_size, self.z_dim)), volatile=True)\n",
    "\n",
    "    def train(self):\n",
    "        self.train_hist = {}\n",
    "        self.train_hist['D_loss'] = []\n",
    "        self.train_hist['G_loss'] = []\n",
    "        self.train_hist['per_epoch_time'] = []\n",
    "        self.train_hist['total_time'] = []\n",
    "        \n",
    "        if self.gpu_mode:\n",
    "            self.y_real = Variable(torch.ones(self.batch_size, 1).cuda())\n",
    "            self.y_fake = Variable(torch.zeros(self.batch_size, 1).cuda())\n",
    "        else:\n",
    "            self.y_real = Variable(torch.ones(self.batch_size, 1))\n",
    "            self.y_fake = Variable(torch.zeros(self.batch_size, 1))\n",
    "\n",
    "        self.D.train()  # trainモードに移行（DropoutやBNに影響）\n",
    "        print('training start!!')\n",
    "        start_time = time.time()\n",
    "        for epoch in range(self.epoch):\n",
    "            self.G.train()\n",
    "            epoch_start_time = time.time()\n",
    "            for iter, (x, _) in enumerate(self.data_loader):\n",
    "                if iter == len(self.data_loader.dataset) // self.batch_size:\n",
    "                    break\n",
    "                z = torch.rand((self.batch_size, self.z_dim))\n",
    "                if self.gpu_mode:\n",
    "                    x, z = Variable(x.cuda()), Variable(z.cuda())\n",
    "                else:\n",
    "                    x, z = Variable(x), Variable(z)\n",
    "                \n",
    "                # update D network\n",
    "                self.D_optimizer.zero_grad()\n",
    "                \n",
    "                # xは本物のデータ\n",
    "                # 正解はy_realなのでそれとのlossをとる\n",
    "                D_real = self.D(x)\n",
    "                D_real_loss = self.BCE_loss(D_real, self.y_real)\n",
    "                \n",
    "                # Gはgeneratorが生成した偽物画像のバッチ\n",
    "                # 正解はy_fakeなのでそれとのlossをとる\n",
    "                G = self.G(z)  # (64, 1, 28, 28)\n",
    "                D_fake = self.D(G)\n",
    "                D_fake_loss = self.BCE_loss(D_fake, self.y_fake)\n",
    "\n",
    "                D_loss = D_real_loss + D_fake_loss\n",
    "                self.train_hist['D_loss'].append(D_loss.data[0])\n",
    "                \n",
    "                D_loss.backward()\n",
    "                self.D_optimizer.step()\n",
    "                \n",
    "                # update G network\n",
    "                self.G_optimizer.zero_grad()\n",
    "                G = self.G(z)  # (64, 1, 28, 28)\n",
    "                D_fake = self.D(G)\n",
    "                # Generatorからは偽物画像をy_realと判定してほしい\n",
    "                G_loss = self.BCE_loss(D_fake, self.y_real)\n",
    "                self.train_hist['G_loss'].append(G_loss.data[0])\n",
    "                \n",
    "                G_loss.backward()\n",
    "                self.G_optimizer.step()\n",
    "                \n",
    "                # 100バッチごとに中間結果を出力\n",
    "                if ((iter + 1) % 100) == 0:\n",
    "                    print('Epoch: [%2d] [%4d/%4d] D_loss: %.8f, G_loss: %.8f' %\n",
    "                          (epoch + 1,\n",
    "                           iter + 1,  # 何番目のバッチか？\n",
    "                           len(self.data_loader.dataset) // self.batch_size,  # バッチ数\n",
    "                           D_loss.data[0],\n",
    "                           G_loss.data[0]))\n",
    "                self.train_hist['per_epoch_time'].append(time.time() - epoch_start_time)\n",
    "\n",
    "                # 生成画像を可視化\n",
    "            \n",
    "        self.train_hist['total_time'].append(time.time() - start_time)\n",
    "        print('Avg one epoch time: %.2f, total %d epochs time: %.2f' %\n",
    "              (np.mean(self.train_hist['per_epoch_time']),\n",
    "               self.epoch,\n",
    "               self.train_hist['total_time'][0]))\n",
    "        print('Training finish! ... save training results')\n",
    "    \n",
    "gan = GAN()\n",
    "gan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_network(net):\n",
    "    num_params = 0\n",
    "    for param in net.parameters():\n",
    "        num_params += param.numel()\n",
    "    print(net)\n",
    "    print('Total number of parameters: %d' % num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d = datasets.MNIST('data/mnist', train=True, download=True,\n",
    "                   transform=transforms.Compose([transforms.ToTensor()]))\n",
    "data_loader = DataLoader(d, batch_size=10000, shuffle=True)\n",
    "for iter, (x, _) in enumerate(data_loader):\n",
    "    print(iter, x.size())\n",
    "print(len(data_loader.dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generator (\n",
      "  (fc): Sequential (\n",
      "    (0): Linear (72 -> 1024)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): ReLU ()\n",
      "    (3): Linear (1024 -> 6272)\n",
      "    (4): BatchNorm1d(6272, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (5): ReLU ()\n",
      "  )\n",
      "  (deconv): Sequential (\n",
      "    (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): ReLU ()\n",
      "    (3): ConvTranspose2d(64, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (4): Sigmoid ()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, dataset='mnist'):\n",
    "        super(generator, self).__init__()\n",
    "        if dataset == 'mnist':\n",
    "            self.input_height = 28\n",
    "            self.input_width = 28\n",
    "            self.input_dim = 62 + 10  # zの乱数の次元 + コンテキスト\n",
    "            self.output_dim = 1\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(),\n",
    "            # 入力の1/4のサイズに縮小\n",
    "            # deconvで4倍に拡大するため\n",
    "            nn.Linear(1024, 128 * (self.input_height // 4) * (self.input_width // 4)),\n",
    "            nn.BatchNorm1d(128 * (self.input_height // 4) * (self.input_width // 4)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, self.output_dim, 4, 2, 1),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "        # 層によってパラメータの初期化方法を変える\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        print('before gen cat:', input.size(), label.size())\n",
    "        x = torch.cat([input, label], 1)\n",
    "        print('after gen cat:', x.size())\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 128, (self.input_height // 4), (self.input_width // 4))\n",
    "        x = self.deconv(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "G = generator('mnist')\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator (\n",
      "  (conv): Sequential (\n",
      "    (0): Conv2d(11, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU (0.2)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (4): LeakyReLU (0.2)\n",
      "  )\n",
      "  (fc): Sequential (\n",
      "    (0): Linear (6272 -> 1024)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): LeakyReLU (0.2)\n",
      "    (3): Linear (1024 -> 1)\n",
      "    (4): Sigmoid ()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, dataset='mnist'):\n",
    "        super(discriminator, self).__init__()\n",
    "        if dataset == 'mnist':\n",
    "            self.input_height = 28\n",
    "            self.input_width = 28\n",
    "            self.input_dim = 1 + 10  # 画像 + クラスラベル\n",
    "            self.output_dim = 1\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.input_dim, 64, 4, 2, 1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * (self.input_height // 4) * (self.input_width // 4), 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(1024, self.output_dim),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "        initialize_weights(self)\n",
    "    \n",
    "    def forward(self, input, label):\n",
    "        print('before disc cat:', input.size(), label.size())\n",
    "        x = torch.cat([input, label], 1)\n",
    "        print('after disc cat:', x.size())\n",
    "        x = self.conv(x)\n",
    "        x = x.view(-1, 128 * (self.input_height // 4) * (self.input_width // 4))\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "D = discriminator('mnist')\n",
    "print(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_mnist(dataset):\n",
    "    data_dir = os.path.join(\"./data\", dataset)\n",
    "\n",
    "    def extract_data(filename, num_data, head_size, data_size):\n",
    "        with gzip.open(filename) as bytestream:\n",
    "            bytestream.read(head_size)\n",
    "            buf = bytestream.read(data_size * num_data)\n",
    "            data = np.frombuffer(buf, dtype=np.uint8).astype(np.float)\n",
    "        return data\n",
    "\n",
    "    data = extract_data(data_dir + '/train-images-idx3-ubyte.gz', 60000, 16, 28 * 28)\n",
    "    trX = data.reshape((60000, 28, 28, 1))\n",
    "\n",
    "    data = extract_data(data_dir + '/train-labels-idx1-ubyte.gz', 60000, 8, 1)\n",
    "    trY = data.reshape((60000))\n",
    "\n",
    "    data = extract_data(data_dir + '/t10k-images-idx3-ubyte.gz', 10000, 16, 28 * 28)\n",
    "    teX = data.reshape((10000, 28, 28, 1))\n",
    "\n",
    "    data = extract_data(data_dir + '/t10k-labels-idx1-ubyte.gz', 10000, 8, 1)\n",
    "    teY = data.reshape((10000))\n",
    "\n",
    "    trY = np.asarray(trY).astype(np.int)\n",
    "    teY = np.asarray(teY)\n",
    "\n",
    "    X = np.concatenate((trX, teX), axis=0)\n",
    "    y = np.concatenate((trY, teY), axis=0).astype(np.int)\n",
    "\n",
    "    seed = 547\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(X)\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(y)\n",
    "\n",
    "    y_vec = np.zeros((len(y), 10), dtype=np.float)\n",
    "    for i, label in enumerate(y):\n",
    "        y_vec[i, y[i]] = 1\n",
    "\n",
    "    X = X.transpose(0, 3, 1, 2) / 255.\n",
    "    # y_vec = y_vec.transpose(0, 3, 1, 2)\n",
    "\n",
    "    X = torch.from_numpy(X).type(torch.FloatTensor)\n",
    "    y_vec = torch.from_numpy(y_vec).type(torch.FloatTensor)\n",
    "    return X, y_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start!!\n",
      "before disc cat: torch.Size([64, 1, 28, 28]) torch.Size([64, 10, 28, 28])\n",
      "after disc cat: torch.Size([64, 11, 28, 28])\n",
      "before cat: torch.Size([64, 62]) torch.Size([64, 10])\n",
      "after cat: torch.Size([64, 72])\n",
      "before disc cat: torch.Size([64, 1, 28, 28]) torch.Size([64, 10, 28, 28])\n",
      "after disc cat: torch.Size([64, 11, 28, 28])\n",
      "before cat: torch.Size([64, 62]) torch.Size([64, 10])\n",
      "after cat: torch.Size([64, 72])\n",
      "before disc cat: torch.Size([64, 1, 28, 28]) torch.Size([64, 10, 28, 28])\n",
      "after disc cat: torch.Size([64, 11, 28, 28])\n",
      "before disc cat: torch.Size([64, 1, 28, 28]) torch.Size([64, 10, 28, 28])\n",
      "after disc cat: torch.Size([64, 11, 28, 28])\n",
      "before cat: torch.Size([64, 62]) torch.Size([64, 10])\n",
      "after cat: torch.Size([64, 72])\n",
      "before disc cat: torch.Size([64, 1, 28, 28]) torch.Size([64, 10, 28, 28])\n",
      "after disc cat: torch.Size([64, 11, 28, 28])\n",
      "before cat: torch.Size([64, 62]) torch.Size([64, 10])\n",
      "after cat: torch.Size([64, 72])\n",
      "before disc cat: torch.Size([64, 1, 28, 28]) torch.Size([64, 10, 28, 28])\n",
      "after disc cat: torch.Size([64, 11, 28, 28])\n",
      "before disc cat: torch.Size([64, 1, 28, 28]) torch.Size([64, 10, 28, 28])\n",
      "after disc cat: torch.Size([64, 11, 28, 28])\n",
      "before cat: torch.Size([64, 62]) torch.Size([64, 10])\n",
      "after cat: torch.Size([64, 72])\n",
      "before disc cat: torch.Size([64, 1, 28, 28]) torch.Size([64, 10, 28, 28])\n",
      "after disc cat: torch.Size([64, 11, 28, 28])\n",
      "before cat: torch.Size([64, 62]) torch.Size([64, 10])\n",
      "after cat: torch.Size([64, 72])\n",
      "before disc cat: torch.Size([64, 1, 28, 28]) torch.Size([64, 10, 28, 28])\n",
      "after disc cat: torch.Size([64, 11, 28, 28])\n",
      "before disc cat: torch.Size([64, 1, 28, 28]) torch.Size([64, 10, 28, 28])\n",
      "after disc cat: torch.Size([64, 11, 28, 28])\n",
      "before cat: torch.Size([64, 62]) torch.Size([64, 10])\n",
      "after cat: torch.Size([64, 72])\n",
      "before disc cat: torch.Size([64, 1, 28, 28]) torch.Size([64, 10, 28, 28])\n",
      "after disc cat: torch.Size([64, 11, 28, 28])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-197-060f2ab49539>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mgan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-197-060f2ab49539>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mD_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_fill\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mD_fake_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCE_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/koichiro.mori/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-194-65b75c1693ef>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, label)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after disc cat:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_height\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_width\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/koichiro.mori/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/koichiro.mori/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/koichiro.mori/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/koichiro.mori/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 254\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/koichiro.mori/.pyenv/versions/anaconda3-4.2.0/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, weight, bias, stride, padding, dilation, groups)\u001b[0m\n\u001b[1;32m     50\u001b[0m     f = ConvNd(_pair(stride), _pair(padding), _pair(dilation), False,\n\u001b[1;32m     51\u001b[0m                _pair(0), groups, torch.backends.cudnn.benchmark, torch.backends.cudnn.enabled)\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class CGAN(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.epoch = 25\n",
    "        self.sample_num = 100\n",
    "        self.batch_size = 64\n",
    "        self.save_dir = 'models'\n",
    "        self.result_dir = 'results'\n",
    "        self.dataset = 'mnist'\n",
    "        self.log_dir = 'logs'\n",
    "        self.model_name = 'GAN'\n",
    "        self.gpu_mode = torch.cuda.is_available()\n",
    "\n",
    "        self.G = generator(self.dataset)\n",
    "        self.D = discriminator(self.dataset)\n",
    "\n",
    "        self.G_optimizer = optim.Adam(self.G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        self.D_optimizer = optim.Adam(self.D.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "        \n",
    "        if self.gpu_mode:\n",
    "            self.G.cuda()\n",
    "            self.D.cuda()\n",
    "            # Dは2値分類なので Binary Cross Entropy Loss を使う\n",
    "            # lossはcuda()する必要あるのか？\n",
    "            self.BCE_loss = nn.BCELoss().cuda()\n",
    "        else:\n",
    "            self.BCE_loss = nn.BCELoss()\n",
    "        \n",
    "#         print(self.G)\n",
    "#         print(self.D)\n",
    "\n",
    "        # load mnist\n",
    "        # data_X: [70000, 1, 28, 28]\n",
    "        # data_Y: [70000, 10]  1-of-K\n",
    "        self.data_X, self.data_Y = load_mnist('mnist')\n",
    "        self.z_dim = 62\n",
    "        self.y_dim = 10\n",
    "\n",
    "    def train(self):\n",
    "        self.y_real = Variable(torch.ones(self.batch_size, 1))\n",
    "        self.y_fake = Variable(torch.zeros(self.batch_size, 1))\n",
    "\n",
    "        self.fill = torch.zeros([10, 10, self.data_X.size()[2], self.data_X.size()[3]])\n",
    "        for i in range(10):\n",
    "            self.fill[i, i, :, :] = 1\n",
    "\n",
    "        self.D.train()\n",
    "        print('training start!!')\n",
    "        for epoch in range(self.epoch):\n",
    "            self.G.train()\n",
    "            for iter in range(len(self.data_X) // self.batch_size):\n",
    "                x = self.data_X[iter * self.batch_size: (iter + 1) * self.batch_size]\n",
    "                z = torch.rand((self.batch_size, self.z_dim))\n",
    "                y_vec = self.data_Y[iter * self.batch_size: (iter + 1) * self.batch_size]\n",
    "                y_fill = self.fill[torch.max(y_vec, 1)[1].squeeze()]\n",
    "                \n",
    "                x, z, y_vec, y_fill = Variable(x), Variable(z), Variable(y_vec), Variable(y_fill)\n",
    "                \n",
    "                # update D network\n",
    "                self.D_optimizer.zero_grad()\n",
    "\n",
    "                D_real = self.D(x, y_fill)\n",
    "                D_real_loss = self.BCE_loss(D_real, self.y_real)\n",
    "                \n",
    "                G = self.G(z, y_vec)\n",
    "                D_fake = self.D(G, y_fill)\n",
    "                D_fake_loss = self.BCE_loss(D_fake, self.y_fake)\n",
    "\n",
    "                D_loss = D_real_loss + D_fake_loss\n",
    "                D_loss.backward()\n",
    "                self.D_optimizer.step()\n",
    "                \n",
    "                # update G network\n",
    "                self.G_optimizer.zero_grad()\n",
    "                \n",
    "                G = self.G(z, y_vec)\n",
    "                D_fake = self.D(G, y_fill)\n",
    "                G_loss = self.BCE_loss(D_fake, self.y_real)\n",
    "                G_loss.backward()\n",
    "                self.G_optimizer.step()\n",
    "\n",
    "gan = CGAN()\n",
    "gan.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
